{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- may want to consider rebalancing signals to be evenly distributed. For aapl_extended, 27% are 0, 35% are 1, and 38% are 2\n",
    "- -we are having a vanishing gradient issue, since all of our values are very small decimals. Should scale these somehow.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>50ma</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cal_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-03-18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.018868</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>-0.324816</td>\n",
       "      <td>-0.015385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-03-19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>-0.007091</td>\n",
       "      <td>-0.422166</td>\n",
       "      <td>-0.068874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-03-20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.006699</td>\n",
       "      <td>-0.002598</td>\n",
       "      <td>-0.021084</td>\n",
       "      <td>-0.014165</td>\n",
       "      <td>0.346232</td>\n",
       "      <td>-0.036981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-03-23</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.028100</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>-0.053077</td>\n",
       "      <td>-0.009539</td>\n",
       "      <td>0.924736</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-03-24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016577</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066206</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>0.629787</td>\n",
       "      <td>-0.009956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      open      high       low  adj_close    volume      50ma  \\\n",
       "cal_date                                                                        \n",
       "1998-03-18   1.0 -0.018868  0.009367  0.005025   0.022848 -0.324816 -0.015385   \n",
       "1998-03-19   1.0  0.033462  0.000000  0.021538  -0.007091 -0.422166 -0.068874   \n",
       "1998-03-20   1.0 -0.006699 -0.002598 -0.021084  -0.014165  0.346232 -0.036981   \n",
       "1998-03-23   3.0 -0.028100 -0.023074 -0.053077  -0.009539  0.924736 -0.016700   \n",
       "1998-03-24   1.0  0.016577  0.066667  0.066206   0.072047  0.629787 -0.009956   \n",
       "\n",
       "           label  \n",
       "cal_date          \n",
       "1998-03-18     1  \n",
       "1998-03-19     1  \n",
       "1998-03-20     2  \n",
       "1998-03-23     2  \n",
       "1998-03-24     1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cleaned_data(ticker, pred_window):\n",
    "    # Get stock data from csv\n",
    "    stock_data = pd.read_csv('./data/{}_extended.csv'.format(ticker), index_col=0).rename_axis('cal_date').reset_index()\n",
    "    stock_data = stock_data.set_index('cal_date')\n",
    "    \n",
    "    # Generate new columns\n",
    "    stock_data['50ma'] = pd.Series.rolling(stock_data['volume'],50, min_periods=50).mean().round()\n",
    "    stock_data['label'] = stock_data['adj_close'].shift(periods=-pred_window)\n",
    "    stock_data['label'] = stock_data['label'] - stock_data['adj_close']\n",
    "    stock_data['label'] = pd.Series(stock_data['label']/stock_data['adj_close']*100).astype(float)\n",
    "    stock_data['label'] = pd.cut(stock_data['label'], [np.NINF,-2,2,np.inf], labels= [0, 1, 2])\n",
    "    \n",
    "    # Format columns as differences\n",
    "    # NOTE: Date is formatted as the difference in days to keep value ranges lower.\n",
    "    for column in stock_data:\n",
    "        if column not in ['date', 'label']:\n",
    "            stock_data[column] = stock_data[column].pct_change()\n",
    "            \n",
    "    # Drop all cols that contain NAs\n",
    "    stock_data.dropna(inplace = True)\n",
    "    \n",
    "    # Refactor date column to be days since last datatpoint and drop NA's again\n",
    "    stock_data['date'] = stock_data.index\n",
    "    stock_data['date'] = stock_data['date'].apply(lambda x: time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple()), convert_dtype=True)\n",
    "    stock_data['date'] = stock_data['date'].diff()/86400\n",
    "    stock_data['date'] = stock_data['date'].round()\n",
    "    \n",
    "    stock_data = stock_data.loc[:, ['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', '50ma', 'label']]\n",
    "    stock_data.drop(labels='close', inplace = True, axis = 1)\n",
    "    stock_data.dropna(inplace = True)\n",
    "    return stock_data\n",
    "\n",
    "    \n",
    "ticker = 'aapl'\n",
    "pred_window = 5\n",
    "stock_data = get_cleaned_data(ticker, pred_window)\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Data Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open: 0.001029... 0.031274...\n",
      "high: 0.000915... 0.027450...\n",
      "low: 0.000980... 0.029448...\n",
      "adj_close: 0.001400... 0.026786...\n",
      "volume: 0.081624... 0.519295...\n",
      "50ma: 0.000500... 0.017036...\n"
     ]
    }
   ],
   "source": [
    "def analyze_distribution(series):\n",
    "    # return mean and std-dev of series\n",
    "    return np.mean(series), np.std(series)\n",
    "\n",
    "cols = ['open', 'high', 'low', 'adj_close', 'volume', '50ma']\n",
    "for col in cols:\n",
    "    mean, stddev = analyze_distribution(stock_data[col])\n",
    "    print(\"{}: {:.6f}... {:.6f}...\".format(col, mean, stddev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pre-Standardization ------\n",
      "open: 0.001029... 0.031274...\n",
      "high: 0.000915... 0.027450...\n",
      "low: 0.000980... 0.029448...\n",
      "adj_close: 0.001400... 0.026786...\n",
      "volume: 0.081624... 0.519295...\n",
      "50ma: 0.000500... 0.017036...\n",
      "------ Post-Standardization ------\n",
      "open: 0.000000... 1.000000...\n",
      "high: -0.000000... 1.000000...\n",
      "low: -0.000000... 1.000000...\n",
      "adj_close: -0.000000... 1.000000...\n",
      "volume: 0.000000... 1.000000...\n",
      "50ma: 0.000000... 1.000000...\n"
     ]
    }
   ],
   "source": [
    "def standardize_series(series):\n",
    "    return (series - np.mean(series))/np.std(series)\n",
    "\n",
    "# Standardize the numeric columns of our dataset\n",
    "cols = ['open', 'high', 'low', 'adj_close', 'volume', '50ma']\n",
    "\n",
    "# Print out pre-standardized distribution metrics\n",
    "print('------ Pre-Standardization ------')\n",
    "for col in cols:\n",
    "    mean, stddev = analyze_distribution(stock_data[col])\n",
    "    print(\"{}: {:.6f}... {:.6f}...\".format(col, mean, stddev))\n",
    "\n",
    "# Standardize\n",
    "for col in cols:\n",
    "    stock_data[col] = standardize_series(stock_data[col])\n",
    "\n",
    "# Print out post-standardized distribution metrics\n",
    "print('------ Post-Standardization ------')\n",
    "for col in cols:\n",
    "    mean, stddev = analyze_distribution(stock_data[col])\n",
    "    print(\"{}: {:.6f}... {:.6f}...\".format(col, mean, stddev))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5289,)\n",
      "(5289, 7)\n",
      "0.27661183588580074 0.3480809226696918 0.37530724144450744\n"
     ]
    }
   ],
   "source": [
    "# Get input data and labels\n",
    "signals = stock_data['label'].values\n",
    "daily_data = stock_data.drop(['label'], axis=1).values\n",
    "\n",
    "print(signals.shape)\n",
    "print(daily_data.shape)\n",
    "\n",
    "zero = [val for val in signals if val == 0]\n",
    "one = [val for val in signals if val == 1]\n",
    "two = [val for val in signals if val == 2]\n",
    "\n",
    "print(len(zero)/len(signals), len(one)/len(signals), len(two)/len(signals))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(daily_data, labels, input_length=7, sequence_length=10, batch_size=10):\n",
    "    # Get total number of days for which we have data -- only want full batches\n",
    "    days_per_batch = batch_size * sequence_length\n",
    "    total_days = (len(daily_data) // days_per_batch) * days_per_batch\n",
    "    \n",
    "    # Iterate through daily data, at intervals of batch_size X sequence_length\n",
    "    for ii in range(0, total_days, days_per_batch):\n",
    "        \n",
    "        # Get all days in this batch\n",
    "        batch_days = daily_data[ii: ii+days_per_batch]\n",
    "        \n",
    "        # Create the batch/label tensor of the right shape (seq_len x batch_size x input_features)\n",
    "        batch = torch.zeros((sequence_length, batch_size, input_length), dtype=torch.float64)\n",
    "        label_data = []\n",
    "        \n",
    "        # Fill out this batch/labels\n",
    "        for batch_num, jj in enumerate(range(0, len(batch_days), sequence_length)):\n",
    "            sequence_tensor = torch.tensor(batch_days[jj:jj+sequence_length])\n",
    "            batch[:, batch_num] = sequence_tensor\n",
    "            \n",
    "            # Only want labels for day at the end of sequence\n",
    "            label_data.append(labels[jj+sequence_length-1])\n",
    "            \n",
    "        # Fill out label tensor\n",
    "        label_tensor = torch.tensor(label_data)\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and validation sets -- will have testing data be first data\n",
    "# in dataset\n",
    "test_prop = 0.2\n",
    "test_end_idx = int(len(daily_data) * test_prop)\n",
    "\n",
    "# Create testing data\n",
    "test_features = daily_data[:test_end_idx]\n",
    "test_labels = signals[:test_end_idx]\n",
    "\n",
    "# Create training data\n",
    "train_features = daily_data[test_end_idx:]\n",
    "train_labels = signals[test_end_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 7]) tensor([[[ 3.0000e+00, -2.9058e-02, -3.2990e-02, -3.0069e-02, -1.6741e-02,\n",
      "          -3.5673e-01, -2.7282e-02],\n",
      "         [ 3.0000e+00, -1.2868e-02, -4.5579e-03,  1.9589e-02,  3.7944e-03,\n",
      "          -5.4672e-01,  1.1877e-02],\n",
      "         [ 3.0000e+00,  5.1975e-02,  1.3261e-02,  9.6080e-02,  2.1861e-02,\n",
      "          -2.3603e-01,  3.0620e-03],\n",
      "         [ 3.0000e+00, -1.1786e-02,  1.3722e-02, -5.3603e-03,  2.4849e-02,\n",
      "          -2.9750e-02,  6.5802e-03],\n",
      "         [ 3.0000e+00,  3.5673e-02,  3.3670e-03,  2.9412e-03, -3.7197e-02,\n",
      "          -1.7479e-01, -9.1146e-03],\n",
      "         [ 1.0000e+00, -2.3218e-02, -1.7195e-02, -1.2443e-02, -2.6609e-02,\n",
      "           7.3605e-02, -4.7008e-03],\n",
      "         [ 1.0000e+00,  4.1308e-02, -1.6129e-03,  4.7591e-02, -2.0262e-02,\n",
      "           5.0938e-01,  1.2798e-02],\n",
      "         [ 1.0000e+00,  1.0169e-02, -3.9500e-03, -1.1636e-02, -3.0093e-02,\n",
      "          -7.1960e-02,  8.8772e-03],\n",
      "         [ 1.0000e+00,  2.5552e-02,  2.7152e-02,  1.3222e-02,  2.7348e-02,\n",
      "           2.9051e-01,  6.3259e-03],\n",
      "         [ 1.0000e+00, -2.0675e-02,  3.6054e-02,  7.8740e-03,  5.3602e-02,\n",
      "           3.3343e-01,  5.7115e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -2.1804e-02, -1.7484e-02, -1.7715e-02, -5.6984e-03,\n",
      "           4.7940e-01,  1.0713e-02],\n",
      "         [ 1.0000e+00,  7.4488e-03, -6.4103e-03, -4.3580e-02, -4.7510e-02,\n",
      "           2.5910e-01,  1.0441e-02],\n",
      "         [ 1.0000e+00,  8.8933e-03, -1.9389e-03,  6.5491e-03, -1.8991e-02,\n",
      "           8.8569e-02,  6.0606e-03],\n",
      "         [ 1.0000e+00,  3.7567e-02, -2.8201e-03,  9.5808e-03, -7.4675e-03,\n",
      "          -3.0267e-01,  1.0718e-04],\n",
      "         [ 1.0000e+00, -3.8396e-02, -4.0268e-02, -1.2903e-02, -7.0922e-03,\n",
      "           3.7047e-01,  2.1540e-03],\n",
      "         [ 1.0000e+00, -2.1006e-02, -6.5610e-03, -1.2027e-02, -1.1988e-02,\n",
      "          -8.7624e-02, -3.9488e-03],\n",
      "         [ 1.0000e+00, -1.1129e-01, -1.2763e-01, -1.3742e-01, -1.2489e-01,\n",
      "           1.7206e+00,  5.6505e-02],\n",
      "         [ 1.0000e+00, -3.8255e-02,  5.9484e-03, -1.3158e-02,  5.0391e-02,\n",
      "           1.6763e-02,  7.5416e-03],\n",
      "         [ 1.0000e+00,  3.7037e-02, -5.8027e-03,  2.3352e-02, -1.1040e-02,\n",
      "          -1.2439e-01,  2.1664e-03],\n",
      "         [ 1.0000e+00,  6.1928e-02,  8.5358e-03,  1.9176e-02,  1.9659e-02,\n",
      "           2.2577e-01,  9.8562e-03]],\n",
      "\n",
      "        [[ 1.0000e+00, -2.1853e-03, -6.5104e-03,  7.6646e-03, -2.5860e-03,\n",
      "          -2.0338e-01,  1.0374e-03],\n",
      "         [ 1.0000e+00, -5.6839e-02, -4.3779e-02, -2.3028e-02, -1.8053e-02,\n",
      "           5.1280e-01,  2.2317e-02],\n",
      "         [ 1.0000e+00, -1.4936e-01, -1.4522e-01, -1.5516e-01, -1.5036e-01,\n",
      "           3.8377e+00,  9.9560e-02],\n",
      "         [ 1.0000e+00, -3.4483e-02, -2.2059e-02, -5.2195e-02, -3.4460e-02,\n",
      "           8.5574e-01, -3.2553e-03],\n",
      "         [ 1.0000e+00, -1.2918e-02,  3.0303e-02, -4.7534e-03,  3.5996e-02,\n",
      "          -3.4785e-01, -2.0699e-03],\n",
      "         [ 1.0000e+00, -2.5409e-02,  9.9064e-03, -1.6232e-02,  5.6531e-02,\n",
      "           8.0624e-01,  5.6195e-03],\n",
      "         [ 1.0000e+00, -3.9058e-02, -3.9506e-02, -2.8966e-02, -4.0949e-02,\n",
      "          -5.3972e-01,  1.7538e-02],\n",
      "         [ 1.0000e+00,  4.1870e-02, -1.7740e-02, -1.6842e-02, -5.5201e-02,\n",
      "           1.7895e-01, -2.5034e-03],\n",
      "         [ 1.0000e+00, -1.8831e-02,  0.0000e+00, -1.1409e-02, -3.0151e-02,\n",
      "          -2.6309e-01, -2.7580e-03],\n",
      "         [ 1.0000e+00, -2.1206e-02,  1.3021e-03,  2.9268e-02,  1.8008e-02,\n",
      "          -3.1824e-01,  2.9506e-04]],\n",
      "\n",
      "        [[ 1.0000e+00,  5.6943e-03,  1.4854e-02, -1.3870e-02, -2.4665e-02,\n",
      "          -6.1663e-02,  1.5677e-04],\n",
      "         [ 1.0000e+00, -1.9108e-02, -3.3735e-02, -2.8084e-02, -2.7419e-02,\n",
      "          -3.3408e-01,  9.2415e-03],\n",
      "         [ 1.0000e+00, -1.1514e-02,  0.0000e+00, -1.7773e-03, -5.5798e-04,\n",
      "          -7.6798e-01,  1.0239e-02],\n",
      "         [ 1.0000e+00, -5.9524e-04, -1.1567e-03,  2.7534e-02,  3.0880e-02,\n",
      "          -5.4977e-01, -8.4895e-03],\n",
      "         [ 2.0000e+00,  5.3540e-02,  6.0520e-02,  5.7313e-02,  6.7858e-02,\n",
      "          -1.8781e-01,  1.2769e-03],\n",
      "         [ 1.0000e+00,  7.4739e-02,  2.3978e-02,  1.7089e-02, -4.3675e-02,\n",
      "           1.8683e-01,  1.3790e-02],\n",
      "         [ 1.0000e+00, -5.1613e-02, -2.5064e-02, -1.4915e-02, -2.0181e-03,\n",
      "          -3.1147e-01, -2.8013e-03],\n",
      "         [ 1.0000e+00, -3.1480e-02, -2.8094e-02, -1.4989e-02, -1.4412e-03,\n",
      "          -5.6670e-01, -6.8193e-03],\n",
      "         [ 1.0000e+00, -2.4487e-02, -2.7237e-02, -3.2587e-02, -2.3666e-02,\n",
      "          -2.1793e-01, -5.9428e-03],\n",
      "         [ 1.0000e+00,  3.2498e-02, -8.4525e-03, -1.3541e-03, -1.9563e-02,\n",
      "          -9.5152e-02,  4.9597e-04]],\n",
      "\n",
      "        [[ 1.0000e+00, -5.2265e-02, -5.5532e-02, -5.0363e-02, -3.4342e-02,\n",
      "           1.3553e+00,  3.5236e-02],\n",
      "         [ 1.0000e+00, -3.8961e-02,  1.5461e-02, -6.5531e-02,  2.8681e-02,\n",
      "           2.0682e-01,  5.7518e-03],\n",
      "         [ 1.0000e+00, -1.1648e-02, -6.2500e-03, -3.5608e-03, -1.5167e-02,\n",
      "           1.2238e-01,  2.2392e-03],\n",
      "         [ 1.0000e+00,  1.8463e-02,  3.1847e-02,  3.5323e-02,  3.8634e-02,\n",
      "           7.2320e-02, -7.8078e-03],\n",
      "         [ 3.0000e+00,  4.5737e-02, -7.4667e-03, -1.6940e-03, -3.8994e-02,\n",
      "           3.0655e-01,  1.0226e-03],\n",
      "         [ 3.0000e+00, -6.0377e-02, -1.0112e-02, -2.6072e-02,  4.1667e-02,\n",
      "          -3.3258e-01,  3.3680e-03],\n",
      "         [ 3.0000e+00,  3.4014e-03,  1.3184e-03,  5.5058e-03, -2.6607e-03,\n",
      "           1.1861e-01,  1.1258e-02],\n",
      "         [ 3.0000e+00,  1.3831e-03,  3.9229e-02,  4.1304e-02,  4.7408e-02,\n",
      "           3.2381e-01,  2.5674e-03],\n",
      "         [ 3.0000e+00, -1.5604e-02, -2.0000e-02, -1.9649e-02, -3.1842e-02,\n",
      "           1.3942e-01, -8.8419e-03],\n",
      "         [ 3.0000e+00, -2.2951e-02, -1.5082e-02, -4.0678e-03, -7.4294e-04,\n",
      "          -1.2615e-01, -9.8569e-03]]], dtype=torch.float64)\n",
      "torch.Size([10]) tensor([0, 0, 2, 1, 1, 0, 1, 0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test out the dataloader\n",
    "sample_batch, sample_labels = next(iter(dataloader(train_features, train_labels)))\n",
    "print(sample_batch.shape, sample_batch)\n",
    "print(sample_labels.shape, sample_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length = 7,lstm_size = 64, lstm_layers=1, output_size = 3, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Apply Log Softmax to dense output -- sum denominator across columns\n",
    "        logps = F.log_softmax(dense_out, dim=1)\n",
    "        \n",
    "        # Return the final output and the hidden state\n",
    "        return logps, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 7])\n",
      "torch.Size([5, 10, 8])\n",
      "torch.Size([10, 8]) tensor([[ 0.0079, -0.0897,  0.2496, -0.0011,  0.2488, -0.1307,  0.0827, -0.1861],\n",
      "        [ 0.0178, -0.0917,  0.2544, -0.0051,  0.2422, -0.1280,  0.0885, -0.1916],\n",
      "        [-0.0017, -0.0847,  0.2487,  0.0097,  0.2532, -0.1268,  0.0643, -0.1798],\n",
      "        [ 0.0255, -0.0852,  0.2472, -0.0049,  0.2539, -0.1493,  0.0748, -0.1906],\n",
      "        [ 0.0398, -0.0885,  0.2453, -0.0200,  0.2517, -0.1417,  0.0790, -0.2136],\n",
      "        [ 0.0266, -0.0808,  0.2466, -0.0066,  0.2543, -0.1433,  0.0721, -0.2016],\n",
      "        [ 0.0198, -0.0727,  0.2507, -0.0006,  0.2499, -0.1360,  0.0796, -0.1845],\n",
      "        [ 0.0302, -0.1045,  0.2434, -0.0423,  0.2474, -0.1426,  0.0993, -0.2200],\n",
      "        [ 0.0372, -0.0843,  0.2465, -0.0161,  0.2512, -0.1522,  0.0768, -0.2071],\n",
      "        [ 0.0372, -0.0824,  0.2467, -0.0144,  0.2516, -0.1515,  0.0751, -0.2053]],\n",
      "       dtype=torch.float64, grad_fn=<SliceBackward>)\n",
      "tensor([[-0.0000,  0.3159,  0.0655],\n",
      "        [-0.0171,  0.3185,  0.0622],\n",
      "        [-0.0000,  0.3140,  0.0589],\n",
      "        [-0.0207,  0.3123,  0.0624],\n",
      "        [-0.0000,  0.3219,  0.0578],\n",
      "        [-0.0221,  0.3150,  0.0570],\n",
      "        [-0.0207,  0.0000,  0.0572],\n",
      "        [-0.0181,  0.3341,  0.0000],\n",
      "        [-0.0197,  0.3179,  0.0594],\n",
      "        [-0.0197,  0.3165,  0.0580]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[-1.2352, -0.9194, -1.1697],\n",
      "        [-1.2474, -0.9118, -1.1681],\n",
      "        [-1.2324, -0.9184, -1.1735],\n",
      "        [-1.2475, -0.9145, -1.1645],\n",
      "        [-1.2352, -0.9134, -1.1775],\n",
      "        [-1.2480, -0.9108, -1.1688],\n",
      "        [-1.1321, -1.1113, -1.0541],\n",
      "        [-1.2356, -0.8834, -1.2175],\n",
      "        [-1.2482, -0.9105, -1.1691],\n",
      "        [-1.2472, -0.9109, -1.1695]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-1.2352, -0.9194, -1.1697],\n",
      "        [-1.2474, -0.9118, -1.1681],\n",
      "        [-1.2324, -0.9184, -1.1735],\n",
      "        [-1.2475, -0.9145, -1.1645],\n",
      "        [-1.2352, -0.9134, -1.1775],\n",
      "        [-1.2480, -0.9108, -1.1688],\n",
      "        [-1.1321, -1.1113, -1.0541],\n",
      "        [-1.2356, -0.8834, -1.2175],\n",
      "        [-1.2482, -0.9105, -1.1691],\n",
      "        [-1.2472, -0.9109, -1.1695]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = StockClassifier(input_length=7, lstm_size=8, lstm_layers=2, output_size=3, drop_prob=0.1).double()\n",
    "hidden = model.init_hidden(10)\n",
    "logps, _ = model.forward(sample_batch, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockClassifier(\n",
       "  (lstm): LSTM(7, 16, num_layers=2, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure that our model is set to 'double' as our volume value requires Float64\n",
    "model = StockClassifier(input_length=7, lstm_size=16, lstm_layers=2, output_size=3, drop_prob=0.5).double()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch: 1/15... Step: 10... Train Loss: 1.648430... Val Loss: 1.413531... Accuracy: 35.714286%...\n",
      "Epoch: 1/15... Step: 20... Train Loss: 2.854118... Val Loss: 1.308614... Accuracy: 38.095239%...\n",
      "Epoch: 1/15... Step: 30... Train Loss: 1.771108... Val Loss: 1.369148... Accuracy: 39.523810%...\n",
      "Epoch: 1/15... Step: 40... Train Loss: 1.993065... Val Loss: 1.308740... Accuracy: 38.095239%...\n",
      "Epoch: 1/15... Step: 50... Train Loss: 0.771135... Val Loss: 1.303795... Accuracy: 38.571429%...\n",
      "Epoch: 1/15... Step: 60... Train Loss: 1.151010... Val Loss: 1.292120... Accuracy: 39.523810%...\n",
      "Epoch: 1/15... Step: 70... Train Loss: 1.293750... Val Loss: 1.235214... Accuracy: 40.476191%...\n",
      "Epoch: 1/15... Step: 80... Train Loss: 1.469228... Val Loss: 1.282463... Accuracy: 36.190477%...\n",
      "Starting Epoch 2\n",
      "Epoch: 2/15... Step: 10... Train Loss: 1.271814... Val Loss: 1.232590... Accuracy: 36.666667%...\n",
      "Epoch: 2/15... Step: 20... Train Loss: 1.250273... Val Loss: 1.183705... Accuracy: 41.904763%...\n",
      "Epoch: 2/15... Step: 30... Train Loss: 1.078889... Val Loss: 1.205568... Accuracy: 41.428572%...\n",
      "Epoch: 2/15... Step: 40... Train Loss: 1.430192... Val Loss: 1.208750... Accuracy: 38.571429%...\n",
      "Epoch: 2/15... Step: 50... Train Loss: 0.825379... Val Loss: 1.200103... Accuracy: 39.523810%...\n",
      "Epoch: 2/15... Step: 60... Train Loss: 1.212328... Val Loss: 1.217507... Accuracy: 39.523810%...\n",
      "Epoch: 2/15... Step: 70... Train Loss: 1.251586... Val Loss: 1.167414... Accuracy: 41.428572%...\n",
      "Epoch: 2/15... Step: 80... Train Loss: 1.208877... Val Loss: 1.218630... Accuracy: 39.523810%...\n",
      "Starting Epoch 3\n",
      "Epoch: 3/15... Step: 10... Train Loss: 1.348407... Val Loss: 1.186305... Accuracy: 41.428572%...\n",
      "Epoch: 3/15... Step: 20... Train Loss: 1.252251... Val Loss: 1.150298... Accuracy: 42.380953%...\n",
      "Epoch: 3/15... Step: 30... Train Loss: 1.062789... Val Loss: 1.168136... Accuracy: 42.857144%...\n",
      "Epoch: 3/15... Step: 40... Train Loss: 1.306442... Val Loss: 1.174583... Accuracy: 40.000001%...\n",
      "Epoch: 3/15... Step: 50... Train Loss: 0.961406... Val Loss: 1.170207... Accuracy: 42.380953%...\n",
      "Epoch: 3/15... Step: 60... Train Loss: 1.069523... Val Loss: 1.178321... Accuracy: 39.523811%...\n",
      "Epoch: 3/15... Step: 70... Train Loss: 1.176317... Val Loss: 1.129520... Accuracy: 40.952382%...\n",
      "Epoch: 3/15... Step: 80... Train Loss: 1.184459... Val Loss: 1.180539... Accuracy: 40.000001%...\n",
      "Starting Epoch 4\n",
      "Epoch: 4/15... Step: 10... Train Loss: 1.377176... Val Loss: 1.156590... Accuracy: 40.476191%...\n",
      "Epoch: 4/15... Step: 20... Train Loss: 0.944541... Val Loss: 1.135014... Accuracy: 41.904763%...\n",
      "Epoch: 4/15... Step: 30... Train Loss: 1.257229... Val Loss: 1.142687... Accuracy: 43.333334%...\n",
      "Epoch: 4/15... Step: 40... Train Loss: 1.014966... Val Loss: 1.146254... Accuracy: 37.619048%...\n",
      "Epoch: 4/15... Step: 50... Train Loss: 0.878470... Val Loss: 1.145762... Accuracy: 42.857144%...\n",
      "Epoch: 4/15... Step: 60... Train Loss: 1.037601... Val Loss: 1.138369... Accuracy: 42.380953%...\n",
      "Epoch: 4/15... Step: 70... Train Loss: 1.057156... Val Loss: 1.106364... Accuracy: 43.809525%...\n",
      "Epoch: 4/15... Step: 80... Train Loss: 1.353287... Val Loss: 1.156488... Accuracy: 41.904763%...\n",
      "Starting Epoch 5\n",
      "Epoch: 5/15... Step: 10... Train Loss: 1.138961... Val Loss: 1.129311... Accuracy: 41.428572%...\n",
      "Epoch: 5/15... Step: 20... Train Loss: 1.334240... Val Loss: 1.114821... Accuracy: 44.285715%...\n",
      "Epoch: 5/15... Step: 30... Train Loss: 1.121166... Val Loss: 1.125960... Accuracy: 44.761905%...\n",
      "Epoch: 5/15... Step: 40... Train Loss: 1.087990... Val Loss: 1.129458... Accuracy: 41.428572%...\n",
      "Epoch: 5/15... Step: 50... Train Loss: 1.013848... Val Loss: 1.127964... Accuracy: 42.380953%...\n",
      "Epoch: 5/15... Step: 60... Train Loss: 1.386027... Val Loss: 1.117096... Accuracy: 44.761905%...\n",
      "Epoch: 5/15... Step: 70... Train Loss: 1.076951... Val Loss: 1.088777... Accuracy: 44.285715%...\n",
      "Epoch: 5/15... Step: 80... Train Loss: 1.072136... Val Loss: 1.134612... Accuracy: 39.523810%...\n",
      "Starting Epoch 6\n",
      "Epoch: 6/15... Step: 10... Train Loss: 0.977882... Val Loss: 1.114231... Accuracy: 41.428572%...\n",
      "Epoch: 6/15... Step: 20... Train Loss: 1.347286... Val Loss: 1.097514... Accuracy: 46.190477%...\n",
      "Epoch: 6/15... Step: 30... Train Loss: 1.048434... Val Loss: 1.114013... Accuracy: 43.809524%...\n",
      "Epoch: 6/15... Step: 40... Train Loss: 1.115351... Val Loss: 1.118818... Accuracy: 40.952382%...\n",
      "Epoch: 6/15... Step: 50... Train Loss: 1.075137... Val Loss: 1.121096... Accuracy: 42.857144%...\n",
      "Epoch: 6/15... Step: 60... Train Loss: 1.127111... Val Loss: 1.110617... Accuracy: 43.333334%...\n",
      "Epoch: 6/15... Step: 70... Train Loss: 0.965933... Val Loss: 1.088170... Accuracy: 44.285715%...\n",
      "Epoch: 6/15... Step: 80... Train Loss: 1.157629... Val Loss: 1.130719... Accuracy: 39.047620%...\n",
      "Starting Epoch 7\n",
      "Epoch: 7/15... Step: 10... Train Loss: 1.108977... Val Loss: 1.117500... Accuracy: 40.000001%...\n",
      "Epoch: 7/15... Step: 20... Train Loss: 1.154952... Val Loss: 1.099691... Accuracy: 44.761906%...\n",
      "Epoch: 7/15... Step: 30... Train Loss: 1.121236... Val Loss: 1.116184... Accuracy: 42.857144%...\n",
      "Epoch: 7/15... Step: 40... Train Loss: 1.093095... Val Loss: 1.116242... Accuracy: 40.476191%...\n",
      "Epoch: 7/15... Step: 50... Train Loss: 0.940582... Val Loss: 1.112452... Accuracy: 43.333334%...\n",
      "Epoch: 7/15... Step: 60... Train Loss: 1.116005... Val Loss: 1.105725... Accuracy: 40.000001%...\n",
      "Epoch: 7/15... Step: 70... Train Loss: 1.209938... Val Loss: 1.087809... Accuracy: 43.333334%...\n",
      "Epoch: 7/15... Step: 80... Train Loss: 1.492665... Val Loss: 1.124916... Accuracy: 41.904762%...\n",
      "Starting Epoch 8\n",
      "Epoch: 8/15... Step: 10... Train Loss: 1.103586... Val Loss: 1.111457... Accuracy: 40.476191%...\n",
      "Epoch: 8/15... Step: 20... Train Loss: 1.012421... Val Loss: 1.100180... Accuracy: 44.285715%...\n",
      "Epoch: 8/15... Step: 30... Train Loss: 1.098532... Val Loss: 1.114726... Accuracy: 39.047619%...\n",
      "Epoch: 8/15... Step: 40... Train Loss: 1.060999... Val Loss: 1.113915... Accuracy: 40.000001%...\n",
      "Epoch: 8/15... Step: 50... Train Loss: 1.053446... Val Loss: 1.109599... Accuracy: 41.428572%...\n",
      "Epoch: 8/15... Step: 60... Train Loss: 1.071611... Val Loss: 1.107262... Accuracy: 39.047620%...\n",
      "Epoch: 8/15... Step: 70... Train Loss: 1.023414... Val Loss: 1.089653... Accuracy: 43.809524%...\n",
      "Epoch: 8/15... Step: 80... Train Loss: 1.366776... Val Loss: 1.117454... Accuracy: 42.380953%...\n",
      "Starting Epoch 9\n",
      "Epoch: 9/15... Step: 10... Train Loss: 1.190028... Val Loss: 1.101916... Accuracy: 39.047620%...\n",
      "Epoch: 9/15... Step: 20... Train Loss: 1.228738... Val Loss: 1.095069... Accuracy: 45.714287%...\n",
      "Epoch: 9/15... Step: 30... Train Loss: 1.065124... Val Loss: 1.102793... Accuracy: 40.952382%...\n",
      "Epoch: 9/15... Step: 40... Train Loss: 1.180076... Val Loss: 1.104213... Accuracy: 40.476191%...\n",
      "Epoch: 9/15... Step: 50... Train Loss: 1.026377... Val Loss: 1.098061... Accuracy: 45.238096%...\n",
      "Epoch: 9/15... Step: 60... Train Loss: 1.002209... Val Loss: 1.096627... Accuracy: 42.380953%...\n",
      "Epoch: 9/15... Step: 70... Train Loss: 1.099898... Val Loss: 1.083052... Accuracy: 44.285715%...\n",
      "Epoch: 9/15... Step: 80... Train Loss: 1.299797... Val Loss: 1.112606... Accuracy: 43.809525%...\n",
      "Starting Epoch 10\n",
      "Epoch: 10/15... Step: 10... Train Loss: 1.121871... Val Loss: 1.097635... Accuracy: 41.428572%...\n",
      "Epoch: 10/15... Step: 20... Train Loss: 1.194871... Val Loss: 1.087684... Accuracy: 45.714286%...\n",
      "Epoch: 10/15... Step: 30... Train Loss: 1.134440... Val Loss: 1.100180... Accuracy: 42.380953%...\n",
      "Epoch: 10/15... Step: 40... Train Loss: 1.098253... Val Loss: 1.100630... Accuracy: 42.380953%...\n",
      "Epoch: 10/15... Step: 50... Train Loss: 1.035428... Val Loss: 1.101508... Accuracy: 42.380953%...\n",
      "Epoch: 10/15... Step: 60... Train Loss: 1.085300... Val Loss: 1.096267... Accuracy: 42.857144%...\n",
      "Epoch: 10/15... Step: 70... Train Loss: 1.041626... Val Loss: 1.083381... Accuracy: 44.285715%...\n",
      "Epoch: 10/15... Step: 80... Train Loss: 1.098430... Val Loss: 1.108011... Accuracy: 44.761906%...\n",
      "Starting Epoch 11\n",
      "Epoch: 11/15... Step: 10... Train Loss: 1.107212... Val Loss: 1.095985... Accuracy: 41.428572%...\n",
      "Epoch: 11/15... Step: 20... Train Loss: 1.030734... Val Loss: 1.086311... Accuracy: 46.190477%...\n",
      "Epoch: 11/15... Step: 30... Train Loss: 1.180873... Val Loss: 1.098096... Accuracy: 41.428572%...\n",
      "Epoch: 11/15... Step: 40... Train Loss: 1.116977... Val Loss: 1.095202... Accuracy: 42.380953%...\n",
      "Epoch: 11/15... Step: 50... Train Loss: 1.004436... Val Loss: 1.095654... Accuracy: 44.285715%...\n",
      "Epoch: 11/15... Step: 60... Train Loss: 1.167796... Val Loss: 1.091446... Accuracy: 43.333334%...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15... Step: 70... Train Loss: 1.046571... Val Loss: 1.081908... Accuracy: 45.238096%...\n",
      "Epoch: 11/15... Step: 80... Train Loss: 1.132742... Val Loss: 1.099710... Accuracy: 45.714287%...\n",
      "Starting Epoch 12\n",
      "Epoch: 12/15... Step: 10... Train Loss: 1.066684... Val Loss: 1.094053... Accuracy: 40.952382%...\n",
      "Epoch: 12/15... Step: 20... Train Loss: 1.127760... Val Loss: 1.083012... Accuracy: 45.714286%...\n",
      "Epoch: 12/15... Step: 30... Train Loss: 1.105277... Val Loss: 1.093298... Accuracy: 43.809524%...\n",
      "Epoch: 12/15... Step: 40... Train Loss: 0.954556... Val Loss: 1.090942... Accuracy: 44.285715%...\n",
      "Epoch: 12/15... Step: 50... Train Loss: 0.886054... Val Loss: 1.090482... Accuracy: 44.761905%...\n",
      "Epoch: 12/15... Step: 60... Train Loss: 1.016566... Val Loss: 1.085967... Accuracy: 43.333334%...\n",
      "Epoch: 12/15... Step: 70... Train Loss: 1.072147... Val Loss: 1.080289... Accuracy: 45.714286%...\n",
      "Epoch: 12/15... Step: 80... Train Loss: 1.144561... Val Loss: 1.097624... Accuracy: 45.238096%...\n",
      "Starting Epoch 13\n",
      "Epoch: 13/15... Step: 10... Train Loss: 1.155403... Val Loss: 1.094252... Accuracy: 41.904762%...\n",
      "Epoch: 13/15... Step: 20... Train Loss: 1.208962... Val Loss: 1.081062... Accuracy: 46.190477%...\n",
      "Epoch: 13/15... Step: 30... Train Loss: 1.069433... Val Loss: 1.093023... Accuracy: 43.333334%...\n",
      "Epoch: 13/15... Step: 40... Train Loss: 1.055703... Val Loss: 1.091869... Accuracy: 42.380953%...\n",
      "Epoch: 13/15... Step: 50... Train Loss: 0.965808... Val Loss: 1.090820... Accuracy: 43.809525%...\n",
      "Epoch: 13/15... Step: 60... Train Loss: 1.129926... Val Loss: 1.085749... Accuracy: 42.380953%...\n",
      "Epoch: 13/15... Step: 70... Train Loss: 1.042148... Val Loss: 1.076775... Accuracy: 46.190477%...\n",
      "Epoch: 13/15... Step: 80... Train Loss: 1.004352... Val Loss: 1.095241... Accuracy: 44.761905%...\n",
      "Starting Epoch 14\n",
      "Epoch: 14/15... Step: 10... Train Loss: 1.012339... Val Loss: 1.091568... Accuracy: 41.428572%...\n",
      "Epoch: 14/15... Step: 20... Train Loss: 1.071602... Val Loss: 1.080318... Accuracy: 47.619048%...\n",
      "Epoch: 14/15... Step: 30... Train Loss: 1.135275... Val Loss: 1.094865... Accuracy: 42.380953%...\n",
      "Epoch: 14/15... Step: 40... Train Loss: 1.139058... Val Loss: 1.094845... Accuracy: 43.809524%...\n",
      "Epoch: 14/15... Step: 50... Train Loss: 1.066063... Val Loss: 1.092608... Accuracy: 43.809525%...\n",
      "Epoch: 14/15... Step: 60... Train Loss: 0.956058... Val Loss: 1.086773... Accuracy: 42.380953%...\n",
      "Epoch: 14/15... Step: 70... Train Loss: 1.073301... Val Loss: 1.080535... Accuracy: 44.285715%...\n",
      "Epoch: 14/15... Step: 80... Train Loss: 0.967042... Val Loss: 1.101388... Accuracy: 43.333334%...\n",
      "Starting Epoch 15\n",
      "Epoch: 15/15... Step: 10... Train Loss: 1.147202... Val Loss: 1.094710... Accuracy: 41.904763%...\n",
      "Epoch: 15/15... Step: 20... Train Loss: 1.107727... Val Loss: 1.086020... Accuracy: 43.809525%...\n",
      "Epoch: 15/15... Step: 30... Train Loss: 1.114585... Val Loss: 1.097870... Accuracy: 41.904762%...\n",
      "Epoch: 15/15... Step: 40... Train Loss: 1.103657... Val Loss: 1.094753... Accuracy: 43.333334%...\n",
      "Epoch: 15/15... Step: 50... Train Loss: 1.054909... Val Loss: 1.094215... Accuracy: 43.809525%...\n",
      "Epoch: 15/15... Step: 60... Train Loss: 1.138605... Val Loss: 1.088015... Accuracy: 42.380953%...\n",
      "Epoch: 15/15... Step: 70... Train Loss: 1.139660... Val Loss: 1.080459... Accuracy: 46.190477%...\n",
      "Epoch: 15/15... Step: 80... Train Loss: 1.061246... Val Loss: 1.098330... Accuracy: 43.333334%...\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 10\n",
    "seq_length = 5\n",
    "learning_rate = 0.003\n",
    "clip = 5\n",
    "input_length=7\n",
    "\n",
    "print_every = 10\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "training_losses = [x for x in range(epochs)]\n",
    "validation_losses = [x for x in range(epochs)]\n",
    "accuracies = [x for x in range(epochs)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    steps = 0\n",
    "    \n",
    "    for t_batch, t_labels in dataloader(train_features, train_labels, batch_size=batch_size\n",
    "                                        ,input_length=input_length, sequence_length=seq_length):\n",
    "        steps += 1\n",
    "    \n",
    "        # Initialize Hidden/Cell state -- batch size is dynamic to account for batches that are not full\n",
    "        hidden = model.init_hidden(t_batch.shape[1])\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # Set tensors to correct device -- GPU or CPU\n",
    "        t_batch, t_labels = t_batch.to(device), t_labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run data through model -- output is output and new hidden/cell state\n",
    "        output, hidden = model(t_batch, hidden)\n",
    "        \n",
    "        # Calculate loss and perform back prop -- clip grads if necessary\n",
    "        loss = criterion(output, t_labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # VALIDATION OF MODEL#\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            accuracy = []\n",
    "            #with torch.no_grad():\n",
    "            for val_batch, val_labels in dataloader(test_features, test_labels, batch_size=batch_size\n",
    "                                                    ,input_length=input_length, sequence_length=seq_length):\n",
    "\n",
    "                #Init hidden state -- again we have a dynamic batch size here\n",
    "                val_hidden = model.init_hidden(val_batch.shape[1])\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "\n",
    "                # Set device for tensors\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in val_hidden:\n",
    "                    each.to(device)\n",
    "\n",
    "                # Run data through network\n",
    "                val_out, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "\n",
    "                # Calculate and record loss\n",
    "                val_loss = criterion(val_out, val_labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                # Calculate accuracy of predictions\n",
    "                ps = torch.exp(val_out)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == val_labels.view(*top_class.shape)\n",
    "                accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "                \n",
    "            # Print out metrics\n",
    "            print('Epoch: {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Step: {}...'.format(steps),\n",
    "                  'Train Loss: {:.6f}...'.format(loss.item()),\n",
    "                  'Val Loss: {:.6f}...'.format(np.mean(val_losses)),\n",
    "                  'Accuracy: {:.6f}%...'.format(np.mean(accuracy) * 100))\n",
    "            \n",
    "            # Record metrics\n",
    "            training_losses[epoch] = loss.item()\n",
    "            validation_losses[epoch] = np.mean(val_losses)\n",
    "            accuracies[epoch] = np.mean(accuracy) * 100\n",
    "            \n",
    "            # Set back to training mode\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop will error out if you try to run it multiple times. This happens because the state of the dataloaders has not changed since the last run, and therefore you'll run out of data very quickly. When this happens, go back to the 'Create test and validation data' cell and re-run. This will reset the data in the generators and allow you to try and train again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss Plot\n",
    "plt.plot(training_losses)#, validation_losses)\n",
    "plt.plot(validation_losses)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy plot\n",
    "plt.plot(accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
