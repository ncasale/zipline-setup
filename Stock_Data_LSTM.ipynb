{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate two csv files for training/testing\n",
    "def get_stock_data(file_name):\n",
    "    ''' \n",
    "        Will divide a dataset into both test and training sets\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        file_name (string): path to the csv file containing all data\n",
    "        percent_test (float): percentage of dataset to set aside for testing\n",
    "        \n",
    "        returns nothing\n",
    "    '''\n",
    "    # Open file and place into dataframe\n",
    "    columns = ['date', 'open', 'high', 'low', 'close', 'volume', '50ma', 'label']\n",
    "    stock_data = pd.read_csv(file_name)[columns]\n",
    "    stock_data['date'] = stock_data['date'].apply(lambda x: time.mktime(datetime.datetime.strptime(x, \"%Y-%m-%d\").timetuple()), convert_dtype=True)\n",
    "    stock_data = stock_data.dropna()\n",
    "    return stock_data\n",
    "\n",
    "# Get stock data from csv\n",
    "stock_data = get_stock_data('aapl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203,)\n",
      "(1203, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get input data and labels\n",
    "signals = stock_data['label'].values\n",
    "daily_data = stock_data.drop(['date', 'label'], axis=1).values\n",
    "\n",
    "print(signals.shape)\n",
    "print(daily_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(daily_data, labels, input_length=6, batch_size=10):\n",
    "    '''Buil Dataloader'''\n",
    "    # Get total number of days for which we have data\n",
    "    total_sequences = len(daily_data)\n",
    "    \n",
    "    #Iterate through daily data, at intervals of batch size\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        \n",
    "        # Get all days in this batch\n",
    "        batch_sequences = daily_data[ii: ii+batch_size]\n",
    "        \n",
    "        # Create the batch tensor -- start by initing zero tensor of right shape\n",
    "        batch = torch.zeros((input_length, len(batch_sequences)), dtype=torch.float64)\n",
    "        # Fill out batch\n",
    "        for batch_num, days in enumerate(batch_sequences):\n",
    "            sequence_tensor = torch.tensor(days)\n",
    "            batch[:, batch_num] = sequence_tensor\n",
    "        \n",
    "        # Shape batch correctly. LSTM layer will expect (seq_len, batch_size, input_features)\n",
    "        batch = torch.transpose(batch,0,1).unsqueeze(0)\n",
    "        \n",
    "        # Create label tensor -- ensure it is the same size as the batch tensor\n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_sequences)])\n",
    "        \n",
    "        yield batch, label_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and validation sets -- will have testing data be first data\n",
    "# in dataset\n",
    "test_prop = 0.2\n",
    "test_end_idx = int(len(daily_data) * test_prop)\n",
    "\n",
    "# Create testing data\n",
    "test_features = daily_data[:test_end_idx]\n",
    "test_labels = signals[:test_end_idx]\n",
    "\n",
    "# Create training data\n",
    "train_features = daily_data[test_end_idx:]\n",
    "train_labels = signals[test_end_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out batch loader\n",
    "#test_batch, test_labels = next(iter(dataloader(train_features, train_labels, input_length=6, batch_size=10)))\n",
    "\n",
    "# Print out batch, data should be of format Sequence_length X Batch_size X Input_Length.\n",
    "# Each sequence is only one datapoint long, this datapoint has 6 elements along the 3rd axis\n",
    "#print(batch.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_length = 6,lstm_size = 64, lstm_layers=1, output_size = 3, \n",
    "                               drop_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(input_length, lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=False)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        '''\n",
    "            Perform a forward pass through the network\n",
    "            \n",
    "            Args:\n",
    "                nn_input: the batch of input to NN\n",
    "                hidden_state: The LSTM hidden/cell state tuple\n",
    "                \n",
    "            Returns:\n",
    "                logps: log softmax output\n",
    "                hidden_state: the updated hidden/cell state tuple\n",
    "        '''\n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden_state = self.lstm(nn_input, hidden)\n",
    "\n",
    "        # Stack up LSTM outputs -- this gets the final LSTM output for each sequence in the batch\n",
    "        lstm_out = lstm_out[-1, :, :]\n",
    "        \n",
    "        # LSTM -> Dense Layer\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # Apply Log Softmax to dense output -- sum denominator across columns\n",
    "        logps = F.log_softmax(dense_out, dim=1)\n",
    "                \n",
    "        # Return the final output and the hidden state\n",
    "        return logps, hidden_state\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StockClassifier(input_length=6, lstm_size=8, lstm_layers=1, output_size=3, drop_prob=0.1).double()\n",
    "hidden = model.init_hidden(10)\n",
    "logps, _ = model.forward(batch, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockClassifier(\n",
       "  (lstm): LSTM(6, 128, num_layers=2, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ensure that our model is set to 'double' as our volume value requires Float64\n",
    "model = StockClassifier(input_length=6, lstm_size=128, lstm_layers=2, output_size=3, drop_prob=0.2).double()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch: 1/1... Step: 25... Train Loss: 1.060776... Val Loss: 1.123810... Accuracy: 32.500001%...\n",
      "Epoch: 1/1... Step: 50... Train Loss: 0.868174... Val Loss: 1.232418... Accuracy: 35.416667%...\n",
      "Epoch: 1/1... Step: 75... Train Loss: 1.388571... Val Loss: 1.374063... Accuracy: 35.416667%...\n",
      "Epoch: 1/1... Step: 100... Train Loss: 1.399415... Val Loss: 1.220023... Accuracy: 27.916667%...\n",
      "Epoch: 1/1... Step: 125... Train Loss: 1.048194... Val Loss: 1.158235... Accuracy: 35.416667%...\n",
      "Epoch: 1/1... Step: 150... Train Loss: 1.075528... Val Loss: 1.102431... Accuracy: 35.416667%...\n",
      "Epoch: 1/1... Step: 175... Train Loss: 1.043668... Val Loss: 1.116236... Accuracy: 35.416667%...\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 5\n",
    "learning_rate = 0.003\n",
    "clip = 5\n",
    "\n",
    "print_every = 25\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting Epoch {}'.format(epoch+1))\n",
    "    steps = 0\n",
    "    \n",
    "    for train_batch, labels in dataloader(train_features, train_labels, batch_size=batch_size, input_length=6):\n",
    "        steps += 1\n",
    "        \n",
    "        #print(\"Train Batch: \", train_batch.shape, \"Labels Size: \", labels.shape)\n",
    "        \n",
    "        # Initialize Hidden/Cell state -- batch size is dynamic to account for batches that are not full\n",
    "        hidden = model.init_hidden(labels.shape[0])\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # Set tensors to correct device -- GPU or CPU\n",
    "        train_batch, train_labels = train_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run data through model -- output is output and new hidden/cell state\n",
    "        output, hidden = model(train_batch, hidden)\n",
    "        \n",
    "        # Calculate loss and perform back prop -- clip grads if necessary\n",
    "        loss = criterion(output.squeeze(), train_labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Take optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # VALIDATION OF MODEL#\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            accuracy = []\n",
    "            with torch.no_grad():\n",
    "                for val_batch, val_labels in dataloader(test_features, test_labels, batch_size=batch_size, input_length=6):\n",
    "\n",
    "                    #Init hidden state -- again we have a dynamic batch size here\n",
    "                    val_hidden = model.init_hidden(val_labels.shape[0])\n",
    "                    val_hidden = tuple([each.data for each in val_hidden])\n",
    "\n",
    "                    # Set device for tensors\n",
    "                    val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                    for each in val_hidden:\n",
    "                        each.to(device)\n",
    "\n",
    "                    # Run data through network\n",
    "                    val_out, val_hidden = model(val_batch, val_hidden)\n",
    "\n",
    "                    # Calculate and record loss\n",
    "                    val_loss = criterion(val_out, val_labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # Calculate accuracy of predictions\n",
    "                    ps = torch.exp(val_out)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == val_labels.view(*top_class.shape)\n",
    "                    accuracy.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "\n",
    "            # Print out metrics\n",
    "            print('Epoch: {}/{}...'.format(epoch+1, epochs),\n",
    "                  'Step: {}...'.format(steps),\n",
    "                  'Train Loss: {:.6f}...'.format(loss.item()),\n",
    "                  'Val Loss: {:.6f}...'.format(np.mean(val_losses)),\n",
    "                  'Accuracy: {:.6f}%...'.format(np.mean(accuracy) * 100))\n",
    "            \n",
    "            # Set back to training mode\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop will error out if you try to run it multiple times. This happens because the state of the dataloaders has not changed since the last run, and therefore you'll run out of data very quickly. When this happens, go back to the 'Create test and validation data' cell and re-run. This will reset the data in the generators and allow you to try and train again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
