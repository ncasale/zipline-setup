{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Empty Pipeline with Screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors.basic import SimpleMovingAverage\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "mean_close_10 = SimpleMovingAverage(\n",
    "    inputs=[USEquityPricing.close],\n",
    "    window_length=10\n",
    ")\n",
    "\n",
    "universe = mean_close_10 > 10\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen=universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zipline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.data import bundles\n",
    "\n",
    "# Name of bundle\n",
    "EOD_BUNDLE_NAME = 'quantopian-quandl'\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Setup the engine to look at the top 500 stocks who have had the highest rolling Average Dollar Volume\n",
    "# over a 120-day window -- This is arbitrary and we can use this parameter to refine which stocks we\n",
    "# want in our universe\n",
    "universe = mean_close_10.top(500) \n",
    "\n",
    "# Select the trading calendar that will be used as a reference when slicing the data\n",
    "trading_calendar = get_calendar('NYSE') \n",
    "\n",
    "# Load the bundle we configured in the previous step into the engine\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Create the engine -- the details of this function are in the utils.py file\n",
    "engine = helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "end_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Universe Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Historical Data\n",
    "\n",
    "Get the OHLC + V data for a given time period. This data will be split into individual dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_field_data(data_portal, trading_calendar, assets, start_date, end_date, field):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# The window of data to obtain\n",
    "start_date = '2015-01-05'\n",
    "end_data = '2017-01-05'\n",
    "\n",
    "# Get the open data\n",
    "open_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                          start_date, end_date, 'open')\n",
    "\n",
    "# Get the high data\n",
    "high_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'high')\n",
    "\n",
    "# Get the low data\n",
    "low_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                         start_date, end_date, 'low')\n",
    "\n",
    "# Get the closing data\n",
    "close_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'close') \n",
    "\n",
    "# Get the volume data\n",
    "volume_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                            start_date, end_date, 'volume')\n",
    "\n",
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "def generate_pf_chart(ticker, historical_dfs):\n",
    "    ''' \n",
    "        This function will create a p&f chart for the given ticker using historical data\n",
    "        \n",
    "        @param ticker: (str) ticker of asset to create P&F chart for\n",
    "        @param historical_dfs: (pd.DataFrame) DataFrame holding historical ticker data\n",
    "        \n",
    "        return: PFChart object from which P&F chart/data can be extracted\n",
    "    \n",
    "    '''\n",
    "    # Set up dataframe instrument\n",
    "    try:\n",
    "        df = historical_dfs[ticker][0]\n",
    "    except:\n",
    "        raise ValueError('Ticker passed does not exist in historical dataset')\n",
    "\n",
    "    # Format date and volume values\n",
    "    df['Date'] = df.index.astype(str)\n",
    "    df['Date'] = df['Date'].str.slice(0,10)\n",
    "    df['Volume'] = df['Volume'].astype(int)\n",
    "\n",
    "    # Tes\n",
    "    dfi = DataframeInstrument(ticker, dataframe=df)\n",
    "\n",
    "    # Create pf chart\n",
    "    chart = PFChart(dfi)\n",
    "    chart.create_chart()\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine OHLC dataframes into singular dataframe\n",
    "Here we combine the four individual dataframes representing OHLC + V data into one historical dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Create dataframe and append blank row\n",
    "historical_dfs = pd.DataFrame(columns=universe_tickers)\n",
    "historical_dfs = historical_dfs.append(pd.Series([np.nan]), ignore_index=True)\n",
    "\n",
    "# Loop through each universe ticker and create a combined dataframe for that ticker\n",
    "for ticker in universe_tickers:\n",
    "    # Get individual series representing the OHLCV data\n",
    "    open_series = open_data[ticker]\n",
    "    high_series = high_data[ticker]\n",
    "    low_series = low_data[ticker]\n",
    "    close_series = close_data[ticker]\n",
    "    volume_series = volume_data[ticker]\n",
    "    \n",
    "    # Combine these series into 1 dataframe\n",
    "    columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    df = pd.concat([open_series,high_series, low_series, close_series, volume_series], axis=1)\n",
    "    df.columns = columns    \n",
    "    \n",
    "    # Save this dataframe to historical_dfs\n",
    "    historical_dfs[ticker] = pd.Series([df])\n",
    "    \n",
    "    \n",
    "# Change the columns to be more human readable\n",
    "columns = helper.beautify_tickers(universe_tickers)\n",
    "historical_dfs.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format P&F Chart Data For Analysis\n",
    "Create a new DataFrame to hold the P&F chart data to be used for model training datasets. \n",
    "Columns '50 Day Moving Average of Volume' and 'Ratio of Volume to The 50 Day Moving Average' are generated (Does not apply to first 50 datapoints). Categorical columns are refactored to be nomial integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(pf_chart):\n",
    "    # Import security data as a dataframe\n",
    "    pf_chart_data = pd.DataFrame.from_dict(pf_chart.chart_meta_data)\n",
    "    pf_chart_data = pf_chart_data.transpose().reset_index(drop=True)\n",
    "    pf_chart_data = pf_chart_data.convert_objects(convert_numeric=True).round(2)\n",
    "\n",
    "    # Create a 50 day moving average of the volume and a ratio between the volume and the 50 day moving average\n",
    "    pf_chart_data['50ma'] = pd.Series.rolling(pf_chart_data['volume'],50, min_periods=50).mean().round()\n",
    "    pf_chart_data['volume_ratio'] = (pf_chart_data['volume'] / pf_chart_data['50ma']).round(2)\n",
    "    # Reformat categorical columns to be numeric (cant decide if these should be numbered by index or polarity ie: buy is 1, sell is -1, none is 0)\n",
    "    pf_chart_data['action'] = pf_chart_data['action'].map({'none': 0, 'x': 1, 'reverse x->o': 2, 'o': 3, 'reverse o->x': 4})\n",
    "    pf_chart_data['direction'] = pf_chart_data['direction'].map({'o': 0, 'x':1})\n",
    "    pf_chart_data['signal'] = pf_chart_data['signal'].map({'sell': 0, 'buy':1}) # We may want to refactor this so none = NaN\n",
    "    \n",
    "    return pf_chart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification and Regression With Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz\n",
    "\n",
    "def CART_Output(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf):\n",
    "    #define training and test samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.25, random_state = 100)\n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = X_test.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    #predict using Gini results\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                                   max_depth=4, min_samples_leaf=5)\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    y_pred = clf_gini.predict(X_test)\n",
    "\n",
    "    #redefine arrays as integer (otherwise error message results from accuracy calculation)\n",
    "    y_test = y_test.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    print (\"Values for factors for Gini -\",title,\":\")\n",
    "    print('')\n",
    "\n",
    "    def measure_performance(X,y,clf_gini, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "        y_pred=clf_gini.predict(X)   \n",
    "        if show_accuracy:\n",
    "            print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\")\n",
    "\n",
    "        if show_classification_report:\n",
    "            print (\"Classification report\")\n",
    "            print (metrics.classification_report(y,y_pred),\"\\n\")\n",
    "\n",
    "        if show_confusion_matrix:\n",
    "            print (\"Confusion matrix\")\n",
    "            print (metrics.confusion_matrix(y,y_pred),\"\\n\")\n",
    "\n",
    "    output = measure_performance(X_train,y_train,clf_gini, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True)\n",
    "    feature_names=df_names\n",
    "    with open(title+\"_\"+str(test_size)+\"_\"+str(random_state)+\"_\"+str(max_depth)+\"_\"+str(min_samples_leaf)+\".txt\", \"w\") as f:\n",
    "        f = tree.export_graphviz(clf_gini, feature_names=df_names, out_file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification and Regression (Dynamic Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "def CART_Test(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf):\n",
    "    #define training and test samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = test_size, random_state = random_state)\n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = X_test.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    #predict using Gini results\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = random_state,\n",
    "                                   max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    y_pred = clf_gini.predict(X_test)\n",
    "\n",
    "    #redefine arrays as integer (otherwise error message results from accuracy calculation)\n",
    "    y_test = y_test.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    def measure_performance(X,y,clf_gini, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "        y_pred=clf_gini.predict(X)   \n",
    "        if show_accuracy:\n",
    "            accuracy = metrics.accuracy_score(y,y_pred)\n",
    "        if show_classification_report:\n",
    "            classification_report = metrics.classification_report(y,y_pred)\n",
    "        if show_confusion_matrix:\n",
    "            confusion_matrix = metrics.confusion_matrix(y,y_pred)\n",
    "        return accuracy, classification_report, confusion_matrix\n",
    "    accuracy, classification_report, confusion_matrix = measure_performance(X_train,y_train,clf_gini, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True)\n",
    "    report_entry = {\n",
    "        'test_size': test_size,\n",
    "        'random_state': random_state, \n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': classification_report, \n",
    "        'confusion_matrix': classification_report\n",
    "    }\n",
    "    return report_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  AAPL  (2017-01-05 o: 115.92 h: 116.86 l: 115.81 c: 116.61)\n",
      "  1.00% box, 3 box reversal, hl method\n",
      "  signal: buy status: bull confirmed\n",
      "\n",
      " 118.87|                                                                    |118.87\n",
      " 117.69|                                                    x   u       x   |117.69\n",
      " 116.52|                                                    x d u d     x   |<< 116.61\n",
      " 115.37|                                                x   u d u d     x   |115.37\n",
      " 114.23|                                                x d u d   d     x   |114.23\n",
      " 113.10|                                                x d A     d     x   |113.10\n",
      " 111.98|                                                x d       B x   u   |111.98\n",
      " 110.87|                                                x         o x C u   |110.87\n",
      " 109.77|                        x   u                   x         o x d u   |109.77\n",
      " 108.68|                        4 d u d             x   x         o x d     |108.68\n",
      " 107.61|                        x d u d             x d u         o u       |107.61\n",
      " 106.54|                        x d   d             x d u         o u       |106.54\n",
      " 105.49|                        x     d             x d u         o u       |105.49\n",
      " 104.44|                        x     o             8 9 u         o         |104.44\n",
      " 103.41|                        x     o             x o u                   |103.41\n",
      " 102.38|                        x     o             u o                     |102.38\n",
      " 101.37|                        x     o             u                       |101.37\n",
      " 100.37|                        x     o     u       u                       |100.37\n",
      "  99.37|                        x     o x   u d x   u                       |99.37\n",
      "  98.39|        x               x     o x 6 u d x d u                       |98.39\n",
      "  97.42|  d     x d             3     o x d u d x d u                       |97.42\n",
      "  96.45|  d u   x d         x   x     o x d   d x d u                       |96.45\n",
      "  95.50|  d u d u d 2       x d u     o x     o 7 o                         |95.50\n",
      "  94.55|  o u d u d u d u   u d u     o x     o u                           |94.55\n",
      "  93.61|  o   d u d u d u d u d u     o x     o u                           |93.61\n",
      "  92.69|      d u d u d u d u o       o u     o u                           |92.69\n",
      "  91.77|      d   d u o   d           o u     o u                           |91.77\n",
      "  90.86|          o                   5 u     o                             |90.86\n",
      "  89.96|                              o u                                   |89.96\n",
      "  89.07|                              o                                     |89.07\n",
      "  88.19|                                                                    |88.19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\Anaconda3\\envs\\zipline\\lib\\site-packages\\ipykernel\\__main__.py:5: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for factors for Gini - PnF of AAPL :\n",
      "\n",
      "Accuracy:0.967 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96        66\n",
      "          1       0.96      0.98      0.97        84\n",
      "\n",
      "avg / total       0.97      0.97      0.97       150\n",
      " \n",
      "\n",
      "Confusion matrix\n",
      "[[63  3]\n",
      " [ 2 82]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "test_size_set = [.20,.25,.30]\n",
    "random_state_set = [0,100]\n",
    "max_depth_set = [3,4,5]\n",
    "min_samples_leaf_set = [5,15,30]\n",
    "\n",
    "# Set input variables to test\n",
    "ticker = 'AAPL'\n",
    "\n",
    "# Numeric values or column names of independent varibles\n",
    "input_cols = [1,5,6,8,9,10,15,16,17]\n",
    "# Numeric value or column name of the dependent variable\n",
    "buy_sell_col = 13\n",
    "# Title of the chart\n",
    "title = 'PnF of ' + ticker \n",
    "\n",
    "# Create a p&f chart for the given ticker using historical data and print the result\n",
    "pf_chart = generate_pf_chart(ticker, historical_dfs)\n",
    "print(pf_chart.chart)\n",
    "\n",
    "# Format P&F Chart Data For Analysis\n",
    "pf_chart_data = format_data(pf_chart)\n",
    "\n",
    "# Set the DataFrame to test\n",
    "df = pf_chart_data\n",
    "#Set the range of the test ()\n",
    "X = df.values[50:len(df)-1,(input_cols)]\n",
    "Y = df.values[51:len(df),buy_sell_col]\n",
    "\n",
    "\n",
    "# Run the CART model generator\n",
    "df_names = list(df.columns.values[input_cols])\n",
    "    \n",
    "accuracy_report = pd.DataFrame(columns = ['test_size', 'random_state', 'max_depth', 'min_samples_leaf', 'accuracy', 'classification_report', 'confusion_matrix'])\n",
    "\n",
    "# Run the CART model generator\n",
    "for test_size, random_state, max_depth, min_samples_leaf in itertools.product(test_size_set, random_state_set, max_depth_set, min_samples_leaf_set):\n",
    "    report_entry = CART_Test(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf)\n",
    "    accuracy_report = accuracy_report.append(report_entry, ignore_index=True)\n",
    "\n",
    "# Reorder accuracy report descending by accuracy\n",
    "accuracy_report = accuracy_report.sort_values(by='accuracy',ascending=False).reset_index()\n",
    "accuracy_report.at[0,'test_size'] = test_size\n",
    "accuracy_report.at[0,'max_depth'] = max_depth\n",
    "accuracy_report.at[0,'min_samples_leaf'] = min_samples_leaf\n",
    "accuracy_report.at[0,'random_state'] = random_state\n",
    "\n",
    "CART_Output(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6 ML models to compare accuracies\n",
    "Using the pf_chart_data DataFrame's independent variables, different non-linear functions can be quickly tested to gauge general accuracy in predicting the dependent variable, buy_sell_col. The box and whisker plots provided in the output graph the different results as well as their accuracy spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "# TODO: Add confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zipline]",
   "language": "python",
   "name": "conda-env-zipline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
