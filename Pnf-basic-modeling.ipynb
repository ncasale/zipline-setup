{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Empty Pipeline with Screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors.basic import SimpleMovingAverage\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "mean_close_10 = SimpleMovingAverage(\n",
    "    inputs=[USEquityPricing.close],\n",
    "    window_length=10\n",
    ")\n",
    "\n",
    "universe = mean_close_10 > 10\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen=universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zipline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.data import bundles\n",
    "\n",
    "# Name of bundle\n",
    "EOD_BUNDLE_NAME = 'quantopian-quandl'\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Setup the engine to look at the top 500 stocks who have had the highest rolling Average Dollar Volume\n",
    "# over a 120-day window -- This is arbitrary and we can use this parameter to refine which stocks we\n",
    "# want in our universe\n",
    "universe = mean_close_10.top(500) \n",
    "\n",
    "# Select the trading calendar that will be used as a reference when slicing the data\n",
    "trading_calendar = get_calendar('NYSE') \n",
    "\n",
    "# Load the bundle we configured in the previous step into the engine\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Create the engine -- the details of this function are in the utils.py file\n",
    "engine = helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "end_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Universe Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Historical Data\n",
    "\n",
    "Get the OHLC + V data for a given time period. This data will be split into individual dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_field_data(data_portal, trading_calendar, assets, start_date, end_date, field):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# The window of data to obtain\n",
    "start_date = '2012-01-05'\n",
    "end_date = '2017-01-05'\n",
    "\n",
    "# Get the open data\n",
    "open_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                          start_date, end_date, 'open')\n",
    "\n",
    "# Get the high data\n",
    "high_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'high')\n",
    "\n",
    "# Get the low data\n",
    "low_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                         start_date, end_date, 'low')\n",
    "\n",
    "# Get the closing data\n",
    "close_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'close') \n",
    "\n",
    "# Get the volume data\n",
    "volume_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                            start_date, end_date, 'volume')\n",
    "\n",
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "def generate_pf_chart(ticker, historical_dfs, duration):\n",
    "    ''' \n",
    "        This function will create a p&f chart for the given ticker using historical data\n",
    "        \n",
    "        @param ticker: (str) ticker of asset to create P&F chart for\n",
    "        @param historical_dfs: (pd.DataFrame) DataFrame holding historical ticker data\n",
    "        \n",
    "        return: PFChart object from which P&F chart/data can be extracted\n",
    "    \n",
    "    '''\n",
    "    # Set up dataframe instrument\n",
    "    try:\n",
    "        df = historical_dfs[ticker][0]\n",
    "    except:\n",
    "        raise ValueError('Ticker passed does not exist in historical dataset')\n",
    "\n",
    "    # Format date and volume values\n",
    "    df['Date'] = df.index.astype(str)\n",
    "    df['Date'] = df['Date'].str.slice(0,10)\n",
    "    df['Volume'] = df['Volume'].astype(int)\n",
    "\n",
    "    # Tes\n",
    "    dfi = DataframeInstrument(ticker, dataframe=df)\n",
    "\n",
    "    # Create pf chart\n",
    "    chart = PFChart(dfi, duration=duration)\n",
    "    chart.create_chart()\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine OHLC dataframes into singular dataframe\n",
    "Here we combine the four individual dataframes representing OHLC + V data into one historical dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper import beautify_tickers\n",
    "\n",
    "def get_hist_data(universe_tickers):\n",
    "    #Create dataframe and append blank row\n",
    "    historical_dfs = pd.DataFrame(columns=universe_tickers)\n",
    "    historical_dfs = historical_dfs.append(pd.Series([np.nan]), ignore_index=True)\n",
    "\n",
    "    # Loop through each universe ticker and create a combined dataframe for that ticker\n",
    "    for ticker in universe_tickers:\n",
    "        # Get individual series representing the OHLCV data\n",
    "        open_series = open_data[ticker]\n",
    "        high_series = high_data[ticker]\n",
    "        low_series = low_data[ticker]\n",
    "        close_series = close_data[ticker]\n",
    "        volume_series = volume_data[ticker]\n",
    "\n",
    "        # Combine these series into 1 dataframe\n",
    "        columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        df = pd.concat([open_series,high_series, low_series, close_series, volume_series], axis=1)\n",
    "        df.columns = columns    \n",
    "\n",
    "        # Save this dataframe to historical_dfs\n",
    "        historical_dfs[ticker] = pd.Series([df])\n",
    "\n",
    "\n",
    "    # Change the columns to be more human readable\n",
    "    columns = beautify_tickers(universe_tickers)\n",
    "    historical_dfs.columns = columns\n",
    "    return historical_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format P&F Chart Data For Analysis\n",
    "Create a new DataFrame to hold the P&F chart data to be used for model training datasets. \n",
    "Columns '50 Day Moving Average of Volume' and 'Ratio of Volume to The 50 Day Moving Average' are generated (Does not apply to first 50 datapoints). Categorical columns are refactored to be nomial integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(pf_chart):\n",
    "    # Import security data as a dataframe\n",
    "    pf_chart_data = pd.DataFrame.from_dict(pf_chart.chart_meta_data)\n",
    "    pf_chart_data = pf_chart_data.transpose().reset_index(drop=True)\n",
    "\n",
    "    # Create a 50 day moving average of the volume and a ratio between the volume and the 50 day moving average\n",
    "    pf_chart_data['50ma'] = pd.Series.rolling(pf_chart_data['volume'],50, min_periods=50).mean().round()\n",
    "    pf_chart_data['volume_ratio'] = (pf_chart_data['volume'] / pf_chart_data['50ma'])\n",
    "    # Reformat categorical columns to be numeric (cant decide if these should be numbered by index or polarity ie: buy is 1, sell is -1, none is 0)\n",
    "    pf_chart_data['action'] = pf_chart_data['action'].map({'none': 0, 'x': 1, 'reverse x->o': 2, 'o': 3, 'reverse o->x': 4})\n",
    "    pf_chart_data['direction'] = pf_chart_data['direction'].map({'o': 0, 'x':1})\n",
    "    pf_chart_data['signal'] = pf_chart_data['signal'].map({'sell': 0, 'buy':1}) # We may want to refactor this so none = NaN\n",
    "    # Clean dataset of rows that contain 'nan'\n",
    "    pf_chart_data.dropna(inplace = True, axis = 'rows')\n",
    "    \n",
    "    return pf_chart_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification and Regression With Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz\n",
    "\n",
    "def measure_performance(X,y,clf_gini, output):\n",
    "    y_pred=clf_gini.predict(X)   \n",
    "    accuracy = metrics.accuracy_score(y,y_pred)\n",
    "    classification_report = metrics.classification_report(y,y_pred)\n",
    "    confusion_matrix = metrics.confusion_matrix(y,y_pred)\n",
    "    if output:\n",
    "        print (\"Accuracy:{0:.3f}\".format(accuracy),\"\\n\")\n",
    "        print (\"Classification report\")\n",
    "        print (classification_report,\"\\n\")\n",
    "        print (\"Confusion matrix\")\n",
    "        print (confusion_matrix,\"\\n\")\n",
    "    return accuracy, classification_report, confusion_matrix\n",
    "\n",
    "def CART(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf, output = False, export = False, report = False):\n",
    "    \n",
    "    #define training and test samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = test_size, random_state = random_state)\n",
    "    X_train = X_train.astype(int)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = X_test.astype(int)\n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    #predict using Gini results\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = random_state,\n",
    "                                   max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    y_pred = clf_gini.predict(X_test)\n",
    "\n",
    "    #redefine arrays as integer (otherwise error message results from accuracy calculation)\n",
    "    y_test = y_test.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    \n",
    "    if report:\n",
    "        accuracy, classification_report, confusion_matrix = measure_performance(X_train,y_train,clf_gini, output)\n",
    "        report_entry = {\n",
    "            'ticker': '',\n",
    "            'test_size': test_size,\n",
    "            'random_state': random_state, \n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': classification_report, \n",
    "            'confusion_matrix': confusion_matrix,\n",
    "            'model': clf_gini\n",
    "        }\n",
    "        return report_entry\n",
    "    else:\n",
    "        measure_performance(X_train,y_train,clf_gini, output)\n",
    "        \n",
    "    if output: \n",
    "        print (\"Values for factors for Gini -\",title,\":\",\"\\n\")\n",
    "    \n",
    "    if export:\n",
    "        feature_names=df_names\n",
    "        with open(\"cart-models/\"+title+\"_\"+str(test_size)+\"_\"+str(random_state)+\"_\"+str(max_depth)+\"_\"+str(min_samples_leaf)+\".txt\", \"w\") as f:\n",
    "            f = tree.export_graphviz(clf_gini, feature_names=df_names, out_file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification and Regression (Dynamic Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pnf import generate_pf_chart\n",
    "from IPython.display import clear_output\n",
    "import itertools\n",
    "\n",
    "def test_models(ticker_set, test_size_set, random_state_set, max_depth_set, min_samples_leaf_set, input_cols, output_col, duration):\n",
    "    accuracy_report = pd.DataFrame(columns = ['ticker', 'test_size', 'random_state', 'max_depth', 'min_samples_leaf', 'accuracy', 'classification_report', 'confusion_matrix', 'model'])\n",
    "    current_ticker = ''\n",
    "    i = 1\n",
    "    \n",
    "    # Test all combinations of the given objects\n",
    "    for ticker, test_size, random_state, max_depth, min_samples_leaf in itertools.product(ticker_set, test_size_set, random_state_set, max_depth_set, min_samples_leaf_set):\n",
    "        if current_ticker != ticker:\n",
    "            current_ticker = ticker\n",
    "            clear_output()\n",
    "            print ('On ticker number {0}/{1}: {2}'.format(str(i),str(len(ticker_set)),ticker))\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Title of the chart\n",
    "        title = 'CART of ' + ticker \n",
    "\n",
    "        # Create a p&f chart for the given ticker using historical data and print the result\n",
    "        pf_chart = generate_pf_chart(ticker, historical_dfs, duration)\n",
    "        # Format P&F Chart Data For Analysis\n",
    "        df = format_data(pf_chart)\n",
    "\n",
    "        #Set the range of the test ()\n",
    "        X = df.values[:len(df)-1,(input_cols)]\n",
    "        Y = df.values[1:len(df),output_col]\n",
    "\n",
    "\n",
    "        # Run the CART model generator\n",
    "        df_names = list(df.columns.values[input_cols])\n",
    "        report_entry = CART(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf,report=True)\n",
    "        report_entry['ticker'] = ticker\n",
    "        accuracy_report = accuracy_report.append(report_entry, ignore_index=True)\n",
    "\n",
    "    # Reorder accuracy report descending by accuracy\n",
    "    accuracy_report = accuracy_report.sort_values(by='accuracy',ascending=False).reset_index(drop=True)\n",
    "    clear_output()\n",
    "    print('Done!')\n",
    "    return accuracy_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On ticker number 1/27: AAPL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-10484983eb16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Create models for each combination of variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0maccuracy_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-f1b68f6660e6>\u001b[0m in \u001b[0;36mtest_models\u001b[1;34m(ticker_set, test_size_set, random_state_set, max_depth_set, min_samples_leaf_set, input_cols, output_col, duration)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Create a p&f chart for the given ticker using historical data and print the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mpf_chart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_pf_chart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistorical_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Format P&F Chart Data For Analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpf_chart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-311b384b5c31>\u001b[0m in \u001b[0;36mgenerate_pf_chart\u001b[1;34m(ticker, historical_dfs, duration)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;31m# Create pf chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPFChart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_chart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\zipline-setup\\pypf\\chart.py\u001b[0m in \u001b[0;36mcreate_chart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_chart_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_chart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_chart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\zipline-setup\\pypf\\chart.py\u001b[0m in \u001b[0;36m_get_chart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m                         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0mchart\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'  '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mchart\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'   |'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscale_right\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "historical_dfs = get_hist_data(universe_tickers)\n",
    "\n",
    "# Set the tickers and parameter values to be tested \n",
    "ticker_set = ['AAPL', 'MMM', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'XOM', 'GS', 'HD' , 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'UTX', 'UNH', 'VZ', 'V', 'WMT', 'WBA']\n",
    "duration = 5 # Number of years to graph \n",
    "\n",
    "# Set model parameters to test\n",
    "test_size_set = [.20,.25,.30] # Percent of the data that is used as training data (Range = 0:1) \n",
    "random_state_set = [0,100] # Percent of the data that is randomized in the training/testing data (Range = 0:100)\n",
    "max_depth_set = [3,4,5] # Max number of layers in the model (Range = 1:inf)\n",
    "min_samples_leaf_set = [5,15,30] # Minimum number of samples in a group to split on (Range = 1:inf)\n",
    "\n",
    "# Set input variables to test\n",
    "input_cols = [1,5,6,8,9,10,15,16,17] # Numeric values or column names of independent varibles\n",
    "output_col = 13 # Numeric value or column name of the dependent variable\n",
    "\n",
    "# Create models for each combination of variables\n",
    "accuracy_report = test_models(ticker_set, test_size_set, random_state_set, max_depth_set, min_samples_leaf_set, input_cols, output_col, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>test_size</th>\n",
       "      <th>random_state</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>classification_report</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.930095</td>\n",
       "      <td>precision    recall  f1-score   s...</td>\n",
       "      <td>[[175, 42], [17, 610]]</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>BA</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.824645</td>\n",
       "      <td>precision    recall  f1-score   s...</td>\n",
       "      <td>[[209, 61], [87, 487]]</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.817536</td>\n",
       "      <td>precision    recall  f1-score   s...</td>\n",
       "      <td>[[232, 110], [44, 458]]</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker  test_size random_state max_depth min_samples_leaf  accuracy  \\\n",
       "0     MMM        0.3            0         5                5  0.930095   \n",
       "48     BA        0.3            0         5                5  0.824645   \n",
       "49   AAPL        0.3          100         5                5  0.817536   \n",
       "\n",
       "                                classification_report  \\\n",
       "0                precision    recall  f1-score   s...   \n",
       "48               precision    recall  f1-score   s...   \n",
       "49               precision    recall  f1-score   s...   \n",
       "\n",
       "           confusion_matrix                                              model  \n",
       "0    [[175, 42], [17, 610]]  DecisionTreeClassifier(class_weight=None, crit...  \n",
       "48   [[209, 61], [87, 487]]  DecisionTreeClassifier(class_weight=None, crit...  \n",
       "49  [[232, 110], [44, 458]]  DecisionTreeClassifier(class_weight=None, crit...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the best performing model paramters for each stock\n",
    "ticker_report = accuracy_report.drop_duplicates(subset = 'ticker')\n",
    "\n",
    "for index, row in ticker_report.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    test_size = row['test_size']\n",
    "    max_depth = row['max_depth']\n",
    "    random_state = row['random_state']\n",
    "    min_samples_leaf = row['min_samples_leaf']\n",
    "    min_samples_leaf = row['min_samples_leaf']\n",
    "    \n",
    "    # Recreate the analysis DataFrame using the best performer\n",
    "    pf_chart = generate_pf_chart(ticker, historical_dfs, duration)\n",
    "    df = format_data(pf_chart)\n",
    "\n",
    "    X = df.values[:len(df)-1,(input_cols)]\n",
    "    Y = df.values[1:len(df),output_col]\n",
    " \n",
    "    # Run the CART model generator\n",
    "    title = 'PnF of ' + ticker \n",
    "    df_names = list(df.columns.values[input_cols])\n",
    "\n",
    "    CART(title,df,X,Y,df_names,test_size,random_state,max_depth,min_samples_leaf, export = True) \n",
    "ticker_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6 ML models to compare accuracies\n",
    "Using the pf_chart_data DataFrame's independent variables, different non-linear functions can be quickly tested to gauge general accuracy in predicting the dependent variable, buy_sell_col. The box and whisker plots provided in the output graph the different results as well as their accuracy spreads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-cf3c64b711c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "# TODO: Add confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0c31c1f712d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zipline]",
   "language": "python",
   "name": "conda-env-zipline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
