{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Empty Pipeline with Screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors.basic import SimpleMovingAverage\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "mean_close_10 = SimpleMovingAverage(\n",
    "    inputs=[USEquityPricing.close],\n",
    "    window_length=10\n",
    ")\n",
    "\n",
    "universe = mean_close_10 > 10\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen=universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zipline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.data import bundles\n",
    "\n",
    "# Name of bundle\n",
    "EOD_BUNDLE_NAME = 'quantopian-quandl'\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Setup the engine to look at the top 500 stocks who have had the highest rolling Average Dollar Volume\n",
    "# over a 120-day window -- This is arbitrary and we can use this parameter to refine which stocks we\n",
    "# want in our universe\n",
    "universe = mean_close_10.top(500) \n",
    "\n",
    "# Select the trading calendar that will be used as a reference when slicing the data\n",
    "trading_calendar = get_calendar('NYSE') \n",
    "\n",
    "# Load the bundle we configured in the previous step into the engine\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Create the engine -- the details of this function are in the utils.py file\n",
    "engine = helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Pipeline as Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find `dot` graph layout program. Make sure Graphviz is installed and `dot` is on your path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\trading_calendars\\utils\\memoize.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\weakref.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: <weakref at 0x000001D938D12228; to 'TermGraph' at 0x000001D938D62048>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\visualize.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(g, out, format_, include_asset_exists)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    675\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    956\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    958\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-28e723b8cf17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Render the pipeline as a DAG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\pipeline.py\u001b[0m in \u001b[0;36mshow_graph\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mRender\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mPipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mDAG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mParameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\pipeline.py\u001b[0m in \u001b[0;36mshow_graph\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_simple_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAssetExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpng\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\trading_calendars\\utils\\memoize.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\graph.py\u001b[0m in \u001b[0;36msvg\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mlazyval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msvg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdisplay_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\visualize.py\u001b[0m in \u001b[0;36mdisplay_graph\u001b[1;34m(g, format, include_asset_exists)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_asset_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude_asset_exists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdisplay_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\zipline\\lib\\site-packages\\zipline\\pipeline\\visualize.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(g, out, format_, include_asset_exists)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             raise RuntimeError(\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[1;34m\"Couldn't find `dot` graph layout program. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[1;34m\"Make sure Graphviz is installed and `dot` is on your path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Couldn't find `dot` graph layout program. Make sure Graphviz is installed and `dot` is on your path."
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "end_date = pd.Timestamp('2017-01-05', tz = 'utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Universe Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Historical Data\n",
    "\n",
    "Get the OHLC + V data for a given time period. This data will be split into individual dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_field_data(data_portal, trading_calendar, assets, start_date, end_date, field):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# The window of data to obtain\n",
    "start_date = '2015-01-05'\n",
    "end_data = '2017-01-05'\n",
    "\n",
    "# Get the open data\n",
    "open_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                          start_date, end_date, 'open')\n",
    "\n",
    "# Get the high data\n",
    "high_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'high')\n",
    "\n",
    "# Get the low data\n",
    "low_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                         start_date, end_date, 'low')\n",
    "\n",
    "# Get the closing data\n",
    "close_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'close') \n",
    "\n",
    "# Get the volume data\n",
    "volume_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                            start_date, end_date, 'volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine OHLC dataframes into singular dataframe\n",
    "Here we combine the four individual dataframes representing OHLC + V data into one historical dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Create dataframe and append blank row\n",
    "historical_dfs = pd.DataFrame(columns=universe_tickers)\n",
    "historical_dfs = historical_dfs.append(pd.Series([np.nan]), ignore_index=True)\n",
    "\n",
    "# Loop through each universe ticker and create a combined dataframe for that ticker\n",
    "for ticker in universe_tickers:\n",
    "    # Get individual series representing the OHLCV data\n",
    "    open_series = open_data[ticker]\n",
    "    high_series = high_data[ticker]\n",
    "    low_series = low_data[ticker]\n",
    "    close_series = close_data[ticker]\n",
    "    volume_series = volume_data[ticker]\n",
    "    \n",
    "    # Combine these series into 1 dataframe\n",
    "    columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    df = pd.concat([open_series,high_series, low_series, close_series, volume_series], axis=1)\n",
    "    df.columns = columns    \n",
    "    \n",
    "    # Save this dataframe to historical_dfs\n",
    "    historical_dfs[ticker] = pd.Series([df])\n",
    "    \n",
    "    \n",
    "# Change the columns to be more human readable\n",
    "columns = helper.beautify_tickers(universe_tickers)\n",
    "historical_dfs.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate P&F Chart from Historical Data\n",
    "Use combined historical dataframe to generate a P&F chart for any ticker included in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "def generate_pf_chart(ticker, historical_dfs):\n",
    "    ''' \n",
    "        This function will create a p&f chart for the given ticker using historical data\n",
    "        \n",
    "        @param ticker: (str) ticker of asset to create P&F chart for\n",
    "        @param historical_dfs: (pd.DataFrame) DataFrame holding historical ticker data\n",
    "        \n",
    "        return: PFChart object from which P&F chart/data can be extracted\n",
    "    \n",
    "    '''\n",
    "    # Set up dataframe instrument\n",
    "    try:\n",
    "        df = historical_dfs[ticker][0]\n",
    "    except:\n",
    "        raise ValueError('Ticker passed does not exist in historical dataset')\n",
    "\n",
    "    # Format date and volume values\n",
    "    df['Date'] = df.index.astype(str)\n",
    "    df['Date'] = df['Date'].str.slice(0,10)\n",
    "    df['Volume'] = df['Volume'].astype(int)\n",
    "\n",
    "    # Tes\n",
    "    dfi = DataframeInstrument(ticker, dataframe=df)\n",
    "\n",
    "    # Create pf chart\n",
    "    chart = PFChart(dfi)\n",
    "    chart.create_chart()\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  AAPL  (2017-01-05 o: 115.92 h: 116.86 l: 115.81 c: 116.61)\n",
      "  1.00% box, 3 box reversal, hl method\n",
      "  signal: buy status: bull confirmed\n",
      "\n",
      " 118.87|                                                                    |118.87\n",
      " 117.69|                                                    x   u       x   |117.69\n",
      " 116.52|                                                    x d u d     x   |<< 116.61\n",
      " 115.37|                                                x   u d u d     x   |115.37\n",
      " 114.23|                                                x d u d   d     x   |114.23\n",
      " 113.10|                                                x d A     d     x   |113.10\n",
      " 111.98|                                                x d       B x   u   |111.98\n",
      " 110.87|                                                x         o x C u   |110.87\n",
      " 109.77|                        x   u                   x         o x d u   |109.77\n",
      " 108.68|                        4 d u d             x   x         o x d     |108.68\n",
      " 107.61|                        x d u d             x d u         o u       |107.61\n",
      " 106.54|                        x d   d             x d u         o u       |106.54\n",
      " 105.49|                        x     d             x d u         o u       |105.49\n",
      " 104.44|                        x     o             8 9 u         o         |104.44\n",
      " 103.41|                        x     o             x o u                   |103.41\n",
      " 102.38|                        x     o             u o                     |102.38\n",
      " 101.37|                        x     o             u                       |101.37\n",
      " 100.37|                        x     o     u       u                       |100.37\n",
      "  99.37|                        x     o x   u d x   u                       |99.37\n",
      "  98.39|        x               x     o x 6 u d x d u                       |98.39\n",
      "  97.42|  d     x d             3     o x d u d x d u                       |97.42\n",
      "  96.45|  d u   x d         x   x     o x d   d x d u                       |96.45\n",
      "  95.50|  d u d u d 2       x d u     o x     o 7 o                         |95.50\n",
      "  94.55|  o u d u d u d u   u d u     o x     o u                           |94.55\n",
      "  93.61|  o   d u d u d u d u d u     o x     o u                           |93.61\n",
      "  92.69|      d u d u d u d u o       o u     o u                           |92.69\n",
      "  91.77|      d   d u o   d           o u     o u                           |91.77\n",
      "  90.86|          o                   5 u     o                             |90.86\n",
      "  89.96|                              o u                                   |89.96\n",
      "  89.07|                              o                                     |89.07\n",
      "  88.19|                                                                    |88.19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test by creating AAPL P&F Chart\n",
    "aapl_pf = generate_pf_chart('AAPL', historical_dfs)\n",
    "print(aapl_pf.chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Relative Strength Chart\n",
    "Use the OHLC data for two stocks to create a RS chart. We will calculate the ratio between the closing price of the two securities on a daily basis, and record this price as all four values of OHLC in the new, combined dataframe. This dataframe can then be used to construct a P&F chart of these ratios, giving us a relative strength chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will generate an rs chart for two given securities\n",
    "def generate_rs_chart(numerator, denominator, historical_dfs):\n",
    "    ''' \n",
    "        This function will generate an RS chart of Numerator/Denominator usingn data from\n",
    "        historical dfs\n",
    "        \n",
    "        @param numerator: (str) Numerator ticker symbol\n",
    "        @param denominator: (str) Denominator ticker symbol\n",
    "        @param historical_dfs: (pd.DataFrame) Dataframe where each col is a ticker, and first entry is OHLC data \n",
    "                                for that ticker.\n",
    "    '''\n",
    "    # Create joint dataframe to hold RS data\n",
    "    rs_df = pd.DataFrame(columns = ['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Set 'Close' equal to ratio between two close prices on a daily basis\n",
    "    rs_df['Close'] = historical_dfs[numerator][0]['Close'] / historical_dfs[denominator][0]['Close']\n",
    "    \n",
    "    # Set other columns equal to same ratio (close/close), since this is the only value that matters\n",
    "    rs_df['High'] = rs_df['Close']\n",
    "    rs_df['Low'] = rs_df['Close']\n",
    "    rs_df['Open'] = rs_df['Close']\n",
    "    rs_df['Volume'] = rs_df['Close']\n",
    "    \n",
    "    # Format date for use with DataframeInstrument\n",
    "    rs_df['Date'] = rs_df.index.astype(str)\n",
    "    rs_df['Date'] = rs_df['Date'].str.slice(0,10)\n",
    "    rs_df['Volume'] = rs_df['Volume'].astype(int)\n",
    "    \n",
    "    \n",
    "    # Create a new DataframeInstrument using this new dataframe\n",
    "    rs_instrument = DataframeInstrument(numerator + '_' + denominator, dataframe=rs_df)\n",
    "    \n",
    "    # Chart the RS data\n",
    "    chart = PFChart(rs_instrument)\n",
    "    chart.create_chart()\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  AAPL_MSFT  (2017-01-05 o: 1.87 h: 1.87 l: 1.87 c: 1.87)\n",
      "  1.00% box, 3 box reversal, hl method\n",
      "  signal: sell status: bear confirmed\n",
      "\n",
      "   2.07|                                                        |2.07\n",
      "   2.05|                  u                               x     |2.05\n",
      "   2.03|              x   u d                             u d   |2.03\n",
      "   2.01|              4 d u d                         x   u d   |2.01\n",
      "   1.99|          x   u d u d                         x d u d   |1.99\n",
      "   1.97|          x d u d u d                         x d A d   |1.97\n",
      "   1.95|  x   x   x d u d u d         x               x d   o   |1.95\n",
      "   1.93|  x d x d 3 d   d u d     x   u d             x     o   |1.93\n",
      "   1.91|  x d x d u     d   o u   x 6 u d u       x   u     o   |1.91\n",
      "   1.89|  x d x d u         o u d x d u d u 7 u   x d u     B   |1.89\n",
      "   1.88|  x d x d u         5 u d x d   d u d u d 8 d u     o   |<< 1.87\n",
      "   1.86|  x d u o           o   d x     o   d u d x d 9     o   |1.86\n",
      "   1.84|    d u                 o x         o u d u o       o   |1.84\n",
      "   1.82|    o u                 o u         o   d u         o   |1.82\n",
      "   1.80|    o u                 o u             d u             |1.80\n",
      "   1.78|    o u                 o u             o u             |1.78\n",
      "   1.77|    2                   o               o u             |1.77\n",
      "   1.75|                                        o u             |1.75\n",
      "   1.73|                                        o u             |1.73\n",
      "   1.71|                                        o               |1.71\n",
      "   1.70|                                                        |1.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a RS chart\n",
    "chart = generate_rs_chart('AAPL', 'MSFT', historical_dfs)\n",
    "print(chart.chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RS Matrix\n",
    "Create a relative strength matrix amongst a given set of tickers. This matrix will generate an RS chart for each ticker against each other ticker, and then determine 1. If the numerator is giving a buy or sell signal, and 2. if the numerator is in a column of Xs or Os. The counts of these signals and columns will be tallied up, and the highest ranked stock will have the most total buy signals and X columns versus the other stocks in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_x(row):\n",
    "    on_x = list(filter(lambda x: x == 'x', row))\n",
    "    return len(on_x)\n",
    "\n",
    "def sum_buy_signals(row):\n",
    "    on_buy_signal = list(filter(lambda x: x == 'buy', row))\n",
    "    return len(on_buy_signal)\n",
    "    \n",
    "def run_rs_matrix(symbols, historical_dfs):\n",
    "    '''\n",
    "        This function will run a relative strength matrix for the symbols passed. \n",
    "        \n",
    "        @param symbols: (list) list of symbols (tickers) to generate chart\n",
    "        @param historical_dfs: (pd.DataFrame) dataframe holding historical ticker data\n",
    "        \n",
    "        return (pd.DataFrame) DataFrame representing an RS matrix\n",
    "    '''\n",
    "    \n",
    "    # Start by ensuring that all tickers within symbols exist in historical df. If not, exclude symbol\n",
    "    valid_symbols = []\n",
    "    for symbol in symbols:\n",
    "        if symbol in historical_dfs.columns:\n",
    "            valid_symbols.append(symbol)\n",
    "            \n",
    "    if len(valid_symbols) >= 2:\n",
    "        # Create a blank matrix to hold the charts\n",
    "        col_matrix = pd.DataFrame(index=valid_symbols, columns=valid_symbols)\n",
    "        signal_matrix = pd.DataFrame(index=valid_symbols, columns=valid_symbols)\n",
    "        for numerator in valid_symbols:\n",
    "            for denominator in valid_symbols:\n",
    "                # Create RS chart with every other chart\n",
    "                chart = generate_rs_chart(numerator, denominator, historical_dfs)\n",
    "                \n",
    "                # Check to see if chart in column of Xs or Os, as well as Buy or Sell signal\n",
    "                key = list(chart._chart_meta_data.keys())[-1]\n",
    "                column_type = chart._chart_meta_data[key]['direction']\n",
    "                signal = chart._chart_meta_data[key]['signal']\n",
    "                \n",
    "                # Save appropriate data to col/signal dataframes\n",
    "                col_matrix.at[numerator, denominator] = column_type\n",
    "                signal_matrix.at[numerator, denominator] = signal\n",
    "                \n",
    "        # Null out the diagonal of matrices, as this is RS versus itself\n",
    "        for symbol in valid_symbols:\n",
    "            col_matrix.at[symbol, symbol] = None\n",
    "            signal_matrix.at[symbol, symbol] = None\n",
    "            \n",
    "        # Get Counts for X columns/Buy Signals\n",
    "        col_matrix['x_count'] = col_matrix.apply(sum_x, axis=1)\n",
    "        signal_matrix['buy_count'] = signal_matrix.apply(sum_buy_signals, axis=1)\n",
    "        \n",
    "        # Combine matrix together\n",
    "        columns = ['x_count', 'buy_count', 'total'] + valid_symbols\n",
    "        rs_matrix = pd.DataFrame(index = valid_symbols, columns=columns)\n",
    "        for symbol in valid_symbols:\n",
    "            rs_matrix[symbol] = col_matrix[symbol] + '_' + signal_matrix[symbol]\n",
    "        rs_matrix['x_count'] = col_matrix['x_count']\n",
    "        rs_matrix['buy_count'] = signal_matrix['buy_count']\n",
    "        rs_matrix['total'] = rs_matrix['buy_count'] + rs_matrix['x_count']\n",
    "        \n",
    "        return rs_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test RS Matrix\n",
    "Test the RS matrix generation function we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_count</th>\n",
       "      <th>buy_count</th>\n",
       "      <th>total</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GE</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GM</th>\n",
       "      <th>NVDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>x_buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>x_buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>x_buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_buy</td>\n",
       "      <td>x_buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GM</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>x_sell</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>o_sell</td>\n",
       "      <td>o_buy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_count  buy_count  total    AAPL    MSFT      GE    TSLA      GM  \\\n",
       "AAPL        2          2      4     NaN  o_sell   x_buy  o_sell  o_sell   \n",
       "MSFT        2          4      6   o_buy     NaN   x_buy  o_sell   o_buy   \n",
       "GE          2          4      6   o_buy   o_buy     NaN  o_sell   x_buy   \n",
       "TSLA        5          5     10   x_buy   x_buy   x_buy     NaN   x_buy   \n",
       "GM          2          0      2  o_sell  o_sell  x_sell  o_sell     NaN   \n",
       "NVDA        0          4      4   o_buy   o_buy   o_buy  o_sell   o_buy   \n",
       "\n",
       "        NVDA  \n",
       "AAPL   x_buy  \n",
       "MSFT   x_buy  \n",
       "GE     x_buy  \n",
       "TSLA   x_buy  \n",
       "GM    x_sell  \n",
       "NVDA     NaN  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the RS Mat\n",
    "run_rs_matrix(['AAPL', 'MSFT', 'GE', 'TSLA', 'GM', 'NVDA'], historical_dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zipline]",
   "language": "python",
   "name": "conda-env-zipline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
