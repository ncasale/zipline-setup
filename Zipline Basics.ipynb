{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Empty Pipeline with Screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resources\n",
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "\n",
    "# Create a screen for our Pipeline\n",
    "universe = AverageDollarVolume(window_length=60) > 50000\n",
    "\n",
    "# Create an empty Pipeline with the given screen\n",
    "pipeline = Pipeline(screen=universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zipline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from zipline.utils.calendars import get_calendar\n",
    "from zipline.data import bundles\n",
    "\n",
    "# Name of bundle\n",
    "EOD_BUNDLE_NAME = 'quantopian-quandl'\n",
    "\n",
    "# Load the data bundle\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Setup the engine to look at the top 500 stocks who have had the highest rolling Average Dollar Volume\n",
    "# over a 120-day window -- This is arbitrary and we can use this parameter to refine which stocks we\n",
    "# want in our universe\n",
    "universe = AverageDollarVolume(window_length=120).top(500) \n",
    "\n",
    "# Select the trading calendar that will be used as a reference when slicing the data\n",
    "trading_calendar = get_calendar('NYSE') \n",
    "\n",
    "# Load the bundle we configured in the previous step into the engine\n",
    "bundle_data = bundles.load(EOD_BUNDLE_NAME)\n",
    "\n",
    "# Create the engine -- the details of this function are in the utils.py file\n",
    "engine = helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Pipeline as Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# Render the pipeline as a DAG\n",
    "pipeline.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the start and end dates\n",
    "start_date = pd.Timestamp('2016-01-05', tz = 'utc')\n",
    "end_date = pd.Timestamp('2016-01-05', tz = 'utc')\n",
    "\n",
    "# Run our pipeline for the given start and end dates\n",
    "pipeline_output = engine.run_pipeline(pipeline, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Universe Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values in index level 1 and save them to a list\n",
    "universe_tickers = pipeline_output.index.get_level_values(1).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "# Create a data portal\n",
    "data_portal = DataPortal(bundle_data.asset_finder,\n",
    "                         trading_calendar = trading_calendar,\n",
    "                         first_trading_day = bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "                         equity_daily_reader = bundle_data.equity_daily_bar_reader,\n",
    "                         adjustment_reader = bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Historical Data\n",
    "\n",
    "Get the OHLC + V data for a given time period. This data will be split into individual dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_field_data(data_portal, trading_calendar, assets, start_date, end_date, field):\n",
    "    \n",
    "    # Set the given start and end dates to Timestamps. The frequency string C is used to\n",
    "    # indicate that a CustomBusinessDay DateOffset is used\n",
    "    end_dt = pd.Timestamp(end_date, tz='UTC', freq='C')\n",
    "    start_dt = pd.Timestamp(start_date, tz='UTC', freq='C')\n",
    "\n",
    "    # Get the locations of the start and end dates\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    # return the historical data for the given window\n",
    "    return data_portal.get_history_window(assets=assets, end_dt=end_dt, bar_count=end_loc - start_loc,\n",
    "                                          frequency='1d',\n",
    "                                          field=field,\n",
    "                                          data_frequency='daily')\n",
    "\n",
    "# The window of data to obtain\n",
    "start_date = '2015-01-05'\n",
    "end_data = '2016-01-05'\n",
    "\n",
    "# Get the open data\n",
    "open_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                          start_date, end_date, 'open')\n",
    "\n",
    "# Get the high data\n",
    "high_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'high')\n",
    "\n",
    "# Get the low data\n",
    "low_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                         start_date, end_date, 'low')\n",
    "\n",
    "# Get the closing data\n",
    "close_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                              start_date, end_date, 'close') \n",
    "\n",
    "# Get the volume data\n",
    "volume_data = get_field_data(data_portal, trading_calendar, universe_tickers,\n",
    "                            start_date, end_date, 'volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine OHLC dataframes into singular dataframe\n",
    "Here we combine the four individual dataframes representing OHLC + V data into one historical dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Create dataframe and append blank row\n",
    "historical_dfs = pd.DataFrame(columns=universe_tickers)\n",
    "historical_dfs = historical_dfs.append(pd.Series([np.nan]), ignore_index=True)\n",
    "\n",
    "# Loop through each universe ticker and create a combined dataframe for that ticker\n",
    "for ticker in universe_tickers:\n",
    "    # Get individual series representing the OHLCV data\n",
    "    open_series = open_data[ticker]\n",
    "    high_series = high_data[ticker]\n",
    "    low_series = low_data[ticker]\n",
    "    close_series = close_data[ticker]\n",
    "    volume_series = volume_data[ticker]\n",
    "    \n",
    "    # Combine these series into 1 dataframe\n",
    "    columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "    df = pd.concat([open_series,high_series, low_series, close_series, volume_series], axis=1)\n",
    "    df.columns = columns    \n",
    "    \n",
    "    # Save this dataframe to historical_dfs\n",
    "    historical_dfs[ticker] = pd.Series([df])\n",
    "    \n",
    "    \n",
    "# Change the columns to be more human readable\n",
    "columns = helper.beautify_tickers(universe_tickers)\n",
    "historical_dfs.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "# Set up dataframe instrument\n",
    "df = historical_dfs['ABM'][0]\n",
    "df.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df['Date'] = df.index.astype(str)\n",
    "df['Date'] = df['Date'].str.slice(0,10)\n",
    "df['Volume'] = df['Volume'].astype(int)\n",
    "dfi = DataframeInstrument('ABM', dataframe=df)\n",
    "\n",
    "# Create pf chart\n",
    "chart = PFChart(dfi)\n",
    "chart.create_chart()\n",
    "print(chart.chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Relative Strength Chart\n",
    "Use the OHLC data for two stocks to create a RS chart. We will calculate the ratio between the closing price of the two securities on a daily basis, and record this price as all four values of OHLC in the new, combined dataframe. This dataframe can then be used to construct a P&F chart of these ratios, giving us a relative strength chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will generate an rs chart for two given securities\n",
    "def generate_rs_chart(numerator, denominator, historical_dfs):\n",
    "    ''' \n",
    "        This function will generate an RS chart of Numerator/Denominator usingn data from\n",
    "        historical dfs\n",
    "        \n",
    "        @param numerator: (str) Numerator ticker symbol\n",
    "        @param denominator: (str) Denominator ticker symbol\n",
    "        @param historical_dfs: (pd.DataFrame) Dataframe where each col is a ticker, and first entry is OHLC data \n",
    "                                for that ticker.\n",
    "    '''\n",
    "    # Create joint dataframe to hold RS data\n",
    "    rs_df = pd.DataFrame(columns = ['Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Set 'Close' equal to ratio between two close prices on a daily basis\n",
    "    rs_df['Close'] = historical_dfs[numerator][0]['close'] / historical_dfs[denominator][0]['close']\n",
    "    \n",
    "    # Set other columns equal to same ratio (close/close), since this is the only value that matters\n",
    "    rs_df['High'] = rs_df['Close']\n",
    "    rs_df['Low'] = rs_df['Close']\n",
    "    rs_df['Open'] = rs_df['Close']\n",
    "    rs_df['Volume'] = rs_df['Close']\n",
    "    \n",
    "    # Format date for use with DataframeInstrument\n",
    "    rs_df['Date'] = rs_df.index.astype(str)\n",
    "    rs_df['Date'] = rs_df['Date'].str.slice(0,10)\n",
    "    rs_df['Volume'] = rs_df['Volume'].astype(int)\n",
    "    \n",
    "    \n",
    "    # Create a new DataframeInstrument using this new dataframe\n",
    "    rs_instrument = DataframeInstrument('AAPL_MSFT', dataframe=rs_df)\n",
    "    \n",
    "    # Chart the RS data\n",
    "    chart = PFChart(rs_instrument)\n",
    "    chart.create_chart()\n",
    "    print(chart.chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a RS chart\n",
    "generate_rs_chart('AAPL', 'MSFT', historical_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Run if changes are made to instrument functions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(pypf.instrument)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
