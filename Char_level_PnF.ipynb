{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate PnF Sequences\n",
    "Iterate through all csvs for S&P 500 and generate their PnF chart. Then serialize the chart's Xs and Os into a string to be used as a sequence in a character-level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnf_char_sequence(chart_meta_data, sequence=''):\n",
    "    '''\n",
    "        In _chart_meta_data, for each date we have a 'move' and a 'direction'. The \n",
    "        direction is either X or O, and the 'move' is the number of that character.\n",
    "        To create sequences, we will iterate through each date in our range, and concat\n",
    "        'move' number of the 'direction' character to our sequence\n",
    "        \n",
    "        Args:\n",
    "            chart_meta_data (dict): the chart meta data dict from PnF object\n",
    "            \n",
    "        Returns:\n",
    "            sequence (string): sequence of Xs and Os representing PnF chart\n",
    "    '''\n",
    "    # Iterate through each day\n",
    "    for k,v in chart_meta_data.items():\n",
    "        direction = v['direction']\n",
    "        move = v['move']\n",
    "        sequence += direction * move\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CSV files for S&P 500 stocks\n",
    "def find_csv_filenames(path_to_dir, suffix=\".csv\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "\n",
    "filenames = find_csv_filenames('./data/', \"_extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [08:59<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all CSVs, convert to PnF, and save Xs and Os to master sequence\n",
    "pnf_sequence = ''\n",
    "\n",
    "for file in tqdm(filenames):\n",
    "    # Get ticker symbol\n",
    "    ticker = file[:file.find('_')].upper()\n",
    "    \n",
    "    # Load ticker data into dataframe\n",
    "    df = pd.read_csv('./data/{}'.format(file))\n",
    "\n",
    "    # Rename columns for use in PnF Library\n",
    "    df.columns = ['Date','Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']\n",
    "\n",
    "    # Create PnF Chart from dataframe\n",
    "    dfi = DataframeInstrument(ticker, dataframe=df)\n",
    "    chart = PFChart(dfi, duration=20.0) #Duration = years of data\n",
    "    chart.create_chart()\n",
    "\n",
    "    # Append sequence        \n",
    "    pnf_sequence = generate_pnf_char_sequence(chart._chart_meta_data, pnf_sequence)\n",
    "    \n",
    "\n",
    "print(len(pnf_sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxoooooxxxxxxxxoooooooooooooooooooooooooooooooooooooooooooooooxxxxxxxxxxxxxxxoooooooooooooxxxxxxxxxxxxxxxxxxxxxxoooooooooxxxxxxxxxxooooooooooooxxxxxxxxxxxooooooooooooooooooooxxxxxxxxxxxxxxxxoooooooxxxxooooooooooxxxxooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooooooooooooxxxxxooooooooooooooooooooxxxoooxxxxxxxxxxxxxxxxxxxxxxoooooooooooxxxxooooooooooooooooxxxxxxxxxxxxooooooooxxxxxxxxxxxxxxxxooooooooooooooooooooooooooooxxxxxxxxxxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooooooooooooxxxxxxxxxxxxxoooooxxxxxxxxoooooooooooooxxxxoooooxxxxxooooooooooxxxxxoooooooooooooxxxxxxxxxxxxxxxxxoooooooooooooxxxxxoooooooxxxxxxxxoooooooooxxxxxoooooooooxxxxxxxxooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooxxxxxxxoooooxxxxxooooooooooooooxxxxxxxxoooooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooooooxxxxxxxxxxxxxoooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooooooooxxxxxxxxxxxxxoooooooooooooooooooooxxxxxxxxxxxxxxooooooooooooxxxxxxxxx\n"
     ]
    }
   ],
   "source": [
    "# Save PnF Sequence to txt file\n",
    "with open('./data/pnf_sequence.txt', 'w') as file:\n",
    "    file.write(pnf_sequence)\n",
    "    \n",
    "print(pnf_sequence[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "Want to map our input characters to integers and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(pnf_sequence))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[char] for char in pnf_sequence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    # Init one-hot array to correct size\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    #Fill appropriate elements with 1s\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshape to get back to desired shape\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test and Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686041 2744168\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(encoded) * test_prop)\n",
    "\n",
    "test_data = encoded[:test_end_idx]\n",
    "train_data = encoded[test_end_idx:]\n",
    "\n",
    "print(len(test_data), len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, lstm_size=256, lstm_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Create Char Dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define LSTM Layer\n",
    "        self.lstm = nn.LSTM(len(self.chars), lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## Define Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define Final Desnse Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden = self.lstm(nn_input, hidden)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\n",
    "        \n",
    "        # LSTM -> Dense -> Dropout\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return dense_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes lstm_layers x batch_size x lstm_size,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(2, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine if we can run on GPU or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_size = 512\n",
    "lstm_layers = 2\n",
    "\n",
    "model = CharRNN(chars, lstm_size, lstm_layers)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "sequence_length = 100\n",
    "clip = 5\n",
    "learn_rate = 0.003\n",
    "print_every = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)\n",
    "\n",
    "steps = 0\n",
    "n_chars = len(model.chars)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Starting Epoch {}\".format(epoch+1))\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through training batches\n",
    "    for train_batch, train_labels in get_batches(train_data, batch_size, sequence_length):\n",
    "        steps += 1\n",
    "        \n",
    "        # Init hidden state\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # One-hot encode data and convert to tensors\n",
    "        train_batch = one_hot_encode(train_batch, n_chars)\n",
    "        train_batch, train_labels = torch.from_numpy(train_batch), torch.from_numpy(train_labels)\n",
    "        \n",
    "        # Ensure that tensors are on correct devices\n",
    "        train_batch, train_labels = train_batch.to(device), train_labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero-out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run data through model and get output/new hidden state\n",
    "        output, hidden = model(train_batch, hidden)\n",
    "        \n",
    "        # Calculate loss and perform backprop -- clip gradients if necessary\n",
    "        loss = criterion(output, train_labels.view(batch_size*sequence_length).long())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        #Take step with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate Model\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            \n",
    "            for val_batch, val_labels in get_batches(test_data, batch_size, sequence_length):\n",
    "                # Init hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # One-hot encode data and convert to tensors\n",
    "                val_batch = one_hot_encode(val_batch, n_chars)\n",
    "                val_batch, val_labels = torch.from_numpy(val_batch), torch.from_numpy(val_labels)\n",
    "                \n",
    "                # Ensure tensors are on correct devices\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run validation data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate and record loss\n",
    "                val_loss = criterion(val_output, val_labels.view(batch_size*sequence_length).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Set back to training mode\n",
    "                model.train()\n",
    "                \n",
    "            # Print out statistics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step: {}...\".format(steps),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
