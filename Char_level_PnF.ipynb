{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pypf.instrument import DataframeInstrument\n",
    "from pypf.chart import PFChart\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate PnF Sequences\n",
    "For a given ticker, generate its PnF chart, and then serialize the Xs and Os into a string to be used as a sequence in a character-level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnf_char_sequence(chart_meta_data):\n",
    "    '''\n",
    "        In _chart_meta_data, for each date we have a 'move' and a 'direction'. The \n",
    "        direction is either X or O, and the 'move' is the number of that character.\n",
    "        To create sequences, we will iterate through each date in our range, and concat\n",
    "        'move' number of the 'direction' character to our sequence\n",
    "        \n",
    "        Args:\n",
    "            chart_meta_data (dict): the chart meta data dict from PnF object\n",
    "            \n",
    "        Returns:\n",
    "            sequence (string): sequence of Xs and Os representing PnF chart\n",
    "    '''\n",
    "    sequence = ''\n",
    "    # Iterate through each day\n",
    "    for k,v in chart_meta_data.items():\n",
    "        direction = v['direction']\n",
    "        move = v['move']\n",
    "        sequence += direction * move\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date   Open   High    Low  Close  Adj_Close    Volume\n",
      "0  1998-01-02  13.63  16.25  13.50  16.25     0.5103   6411700\n",
      "1  1998-01-05  16.50  16.56  15.19  15.88     0.4987   5820300\n",
      "2  1998-01-06  15.94  20.00  14.75  18.94     0.5948  16182800\n",
      "3  1998-01-07  18.81  19.00  17.31  17.50     0.5496   9300200\n",
      "4  1998-01-08  17.44  18.62  16.94  18.19     0.5713   6910900\n",
      "11545 oooooooxxxxxxooooxxxxxxxxxxxoooxxxxxxooooooxxxoooooooxxxxxxooooooxxxxoooooooxxxoooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooxxxxxxxxxxxxxxxxxooooooooooooooxxxxxxxxooooooxxxxxxooooooooxxxxxxoooooooxxxoooooooxxxxxxxxxxxxxxxxxoooooooxxxxxxooooooxxxxoooooooooooxxxxxxxxxxoooxxxxxxxxxxxxxxxxxxxxooooxxxxxxoooooooooxxxxxxxxoooooxxxxoooooooxxxxoooooxxxxxxxxoooxxxxxxxoooooooooxxxxxxxxxxxxxxxxooooxxxxoooooxxxxxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxooooxxxxoooxxxxxooooooxxxxxxxooooooooooooooxxxooooooooooooxxxxxoooooooooooooooxxxxxxxxxxxoooooooxxxxxxxxxxxxxxxoooooooooxxxxxxxxxooooooooxxxxxxxxxxxxxxxxoooooooooxxxxxxxxxxxooooxxxoooxxxxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxxooooooooooxxxxxoooooxxxxxooooxxxxxxoooooooxxxxxxxoooxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooooooooooxxxoooooooooooxxxxxxxxxxooooxxxxxxxoooooooooxxxxxxxxoooxxxxxxxxxxxooooooooooxxxxxxxxooooooooooooooxxxxxxoooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooooooooooxxxxxxxxxxooooooooooooooooooxxxxxxxxxxoooooooxxxxxxxxxxxxxxxxxxoooooooxxxxxxxxxooooooooooxxxxxxxxxxoooooooooxxxxxxxxxxxxxxxxxxxoooooooooxxxxxxxooooooooxxxxxxxoooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooxxxxxoooooooooooooxxxxxxxxxxoooooooooooooooooxxxxxxxxxxxxxooooooooooooooooooooooooxxxxxxxxxxxxooooooooxxxxxxxxxxxxxxxxxooooooooooooxxxxxxxxxxxoooooooooxxxxxxxxooooooooooooooxxxooooooooooooooxxxxxxxxxxoooooooooxxxxxxxxooooooooxxxxoooooooooooooooooooxxxxxxxoooooooooooxxxxxxxxxxoooooooooooxxxxxxxxxxooooooooooooxxxxxxxxxxxxxxxxxxxxxooooooooooxxxxxxxoooooxxxxxxxoooooooooooxxxxxxxxoooooooxxxxxxxxxxxxxxxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxxxxxxxxxxxxxooooooooooooxxxxxxxxoooooooxxxxxxoooooooooxxxxxxxxxxxxxxxxxoooooooxxxxxxooooooooooooxxxxxxxxxooooooooooooooooooxxxxxxxxxxxooooooooooooxxxxxxxxxoooooooooooooooxxxxxxxxxxxxxxoooooooooxxxoooooxxxxxxxxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxxxoooooooooxxxxxxxoooooooooxxxxooooooooxxxxxxxxxxoooxxxxooooooooooooooooooooxxxxxxxxxxoooooooooooooxxxxxxxxxxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxxxxxxxxxxxoooooooooooooooxxxxxxxxoooooooooooooooooxxxxxxxxxxxxxxxxxxxooooooooooooooooooooooooxxxxxxxxxxxxxoooooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooooooooooxxxooooooooxxxxxxxxxxxoooooooooooxxxxxxoooooxxxxxxxooooooooooooooooooooxxxxxxxoooooooooooooooooooooxxxxxxxxxxxxoooooooooooooxxxxxxxxooooooooooxxxxxxxxxooooooxxxxxxoooooxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooooooooxxxxxxxxxoooooooooxxxxxxxxxxxxxxooooooooxxxxxxoooooooooxxxxxxxxxxxxxxxxoooooxxxxxxxxxxxxooooooxxxxooooooxxxxxxxxxxxxxxxooooooooxxxxxxooooooooooxxxxxxxoooooooxxxxxxoooooooooooxxxxxxxxoooooooooxxxxxxxxxoooooooooooxxxxxxxxoooooooooxxxxxxxxoooooooooooooxxxxxxxxxxxxxxxxxoooooxxxxxxxxxxxxoooooooooooooooooooxxxxxxxxxxxxxxxxoooooooooooxxxxxxxxxoooooooxxxxxxxxxxxxxxxxxxxxoooooooooxxxxxxxooooooooxxxxxooooooooooooooooooxxxxxxxxxxxxxxxxxoooooooooooxxxxxxxxxxxxxxooooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooooxxxxxxxxxxxxxooooooxxxxoooooooooxxxxxxooooooooooooxxxxxxxxxxxooooooooooxxxxxooooxxxoooooooooooooooooooooxxxxxxxxoooxxxxxxoooooooooxxxxxxxxxoooooooooooxxxxxxxxxxxxxxxxooooxxxxxxxxxooooooxxxxxxxxxxoooooooxxxooooooooooooooxxxxxxxxxxxxxxxxxxooooooooooooooooooooooooxxxxoooooooooooooxxxxxxxoooxxxxxxooooxxxoooooxxxoooooooooxxxxxxxxxooooooooooxxxxxxxxxoooooooooooooooooooxxxxxxxxxxxooooooooooooxxxxxxxxooooooooooooooxxxxxxxxxxxxxoooooooooooxxxxxxoooooooxxxxxxxxxooooooooxxxxxxxxoooxxxxxxxxxxxxxxxoooooooxxxxxxxxxxxxooooooooxxxxxxxxooooooooooooxxxxxxxxxxxxxxxxoooooooxxxooooooxxxxxxxxxoooooooxxxxxxxxoooxxxxxxxxxxooooooxxxxxoooxxxxxxxxxxxxxxxooooooooxxxooooooooooooxxxxxxxoooooxxxxxxxxxxooooxxxxxxxxxooooooooooooooooxxxxxxoooxxxoooooxxxxxxxxxooooxxxxxxxxoooxxxxxxxxxxxxxoooooooxxxxoooooooxxxxxxxooooooooooooooooxxxxxxxxxxxxxxooooooooooooooooxxxxxxxxxxxxxxxooooooxxxxxxxxooooxxxxxoooooooxxxxooooooxxxxxxxooooxxxxxxoooooxxxxoooxxxxxxxooooooooooooxxxxxoooxxxooooooooxxxxxxxxxooooooxxxxxxxxxxxxooooooooooxxxxxxxoooooooxxxxoooooooooooooooxxxooooooooooooooooooxxxxxxxxxxxxxooooooooooooooooooooxxxxoooooxxxxxoooooooooxxxxxxxxxxoooooxxxxxxxxxxoooooooooxxxxxxxxxooooooooooxxxxxxxxxooooooooooooooooooooooooxxxxoooooooooxxxxxxxxxxxooooooooooxxxxxxxxxoooooxxxxxxxxxxxoooooxxxxxoooooooooooxxxooooooxxxxooooxxxoooxxxoooxxxxxxooooxxxxxoooooooxxxooooooooxxxxxxxxxxxxooooooooxxxxxxxxxxxxxooooooxxxxxxxxxoooxxxxxxxxoooooxxxxxoooooooooooooxxxxxooooxxxxxxxooooooooxxxxxxxxoooooooxxxxxooooooooooxxxxoooxxxxxoooooxxxoooooooooxxxxxxooooooxxxxxxxxxxoooooooxxxooooooxxxoooooxxxxxxxxxxooooooxxxxxoooxxxoooooxxxxxxxoooooooooxxxxxxxxoooxxxxoooooooxxxxxxxoooxxxoooooooxxxxxoooooooooooooxxxxxoooooooxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooxxxxxxxxooooooxxxxxxooooooooooxxxxxoooooooxxxxxxxxxxxxxxxoooxxxxooooooxxxxxxxxxxoooooxxxxooooxxxxxxxxoooxxxxxoooooxxxxxooooooxxxxxxooooooooooxxxxxxxxxxxxoooooooxxxxxxxxxxxxxoooooxxxxoooooxxxxxoooooooooooooxxxxxxooooooxxxxxxxxxxxxxxxoooxxxxxxxxxooooooooooxxxxooooxxxooooxxxxxxxooooooooooxxxxooooooooooooxxxxxxooooxxxxxxxoooooooxxxxooooooxxxxxxxooooooooooxxxxxxxxxxxxxxxxxxxxxoooxxxxxxooooooooooxxxxooooooxxxxxxxxxoooooooooxxxxxxxxxxxxxxxxxxxxxxoooooxxxxxxxxooooooooooxxxxxoooooxxxxxxxxxooooxxxxxxoooooxxxxooooooxxxxxxxxxxxooooooooooooooxxxxoooxxxxxxoooooxxxxxooooxxxoooxxxxxxxxoooxxxxxxxxxxooooxxxxxxxxxxxxoooxxxoooxxxxxoooooooxxxxooooooooooxxxxxooooooooxxxxxxxxxxxxxxxooooooxxxooooooxxxxxxooooxxxxxoooooxxxooooooooxxxxxoooxxxxooooxxxxooooxxxxxxxxxxxxxxxxxxooooxxxxxxxxxxooooxxxxxxxxxoooooooxxxxxxxxxxxxxxxxxxxoooxxxxxxxxoooxxxxxxxxxxxxxxxxxxoooooooxxxxxxoooxxxxxxxxxxxxxxxxxoooxxxxxxxxxxxxoooooooooooxxxxxxooooooxxxxxxxxoooooooooxxxxxooooxxxxoooxxxxoooxxxxxxxxxxooooooooooxxxxxxxxxxxxxxxooooooxxxxxxxxxxxxxxoooxxxxxooooooxxxxxxxxxxxxxxxxxooooooxxxxoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxxxxooooooooooxxxxxxxxxxxxoooooooooxxxxxxxxxxooooooooooooooooooooooooooxxxxxxxxxoooooooxxxxxxxooooooxxxxxoooooooooooxxxxxxoooxxxxoooxxxxxxxxooooxxxxxxxxxxxxooooooooooxxxooooooxxxxxxxxxxxoooooooooxxxxxxxxxxxxxxoooxxxxxxxxooooxxxxxxxxxxxxxooooooxxxxooooxxxxxxxxxxxxxooooxxxxxxxxooooxxxxxooooooxxxxxxxxooooooooooooooxxxxxxxxxxxxoooooxxxxxxxxxxoooxxxxoooooxxxxxxxxxxxxxooooxxxooooxxxoooxxxxxxxxxxxxxxxxxxoooooxxxxxxxxxxxooooooxxxxxoooooxxxxxxxxxxxxxxxxxxxooooooooooooxxxxooooooooooxxxxxxoooooooooooooxxxxoooooooooxxxxxxxxxxxooooxxxxxxxoooooooooooooooxxxxxxxoooooooooooooxxxoooooxxxxxxxxxoooxxxxxxxxxxxxxxxooooxxxoooooooooxxxxxxxoooooxxxxxxxxxxxooooooooooooooooxxxoooooxxxxooooooooxxxxxxoooooooooxxxxxxoooooooxxxxoooxxxxoooooooxxxxxoooooooxxxoooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooxxxxxooooooxxxxxxooooooooooxxxxxxxxxoooooxxxxxxxxxxxxoooxxxxxxooooxxxxxxoooooxxxooooxxxxxoooxxxxxxxxxxoooooxxxxxxxxxxxxxxxxxoooxxxooooooooxxxxxoooooooxxxoooooooooooxxxxxxxxxxxoooxxxxxxxxxxxxxxxooooxxxxoooooooooooooxxxooooooxxxxoooxxxxxxxooooooooxxxxxxoooxxxxxxxxxxxxoooooooxxxxxxxxxxxxxooooxxxxxxxxxxxoooooxxxxxxxxxoooxxxxxxxxxxxxxxoooooooooxxxxxxxoooxxxoooooxxxxoooxxxxxxxxxxxxxxxxxxxoooooooxxxxxxxxxooooooooooooooxxxxxxooooooxxxxxxooooooooooooxxxxxxooooooooooooooxxxxxxxxxxxxxxxxxxxooooooxxxxxxxxxxxxxoooooooooooxxxxxxoooxxxxxxxxxxxxxxxxxoooxxxxxxxxxxxooooooxxxxxxxxxxxxxxxooooxxxxxoooooxxxxxxooooooooooooooooooooooooxxxxxxxxxxxxxxxxooooooooooxxxxxxxooooxxxxxxxooooxxxxxxxxxooooxxxxxxxxxoooooxxxxxxxoooooooooooxxxxxxxxxxxxooooooooooooooooxxxxxxooooooooxxxxxxxooooooxxxxxoooooooooooooxxxxxoooooooooooooooooooooooooooxxxxxxxxxxoooooooooxxxxxxxoooooooooooooooxxxxxxxxxxoooxxxoooooooooooxxxoooxxxxxxxxxxxxooooooooooxxxxxxxooooooxxxxxxxxoooooxxxxxxxxoooxxxxxxxxxxxoooxxxxxxxxxxxxoooooooooxxxxxxxxxxxxxxxooooooxxxxxxxxxxxxxxxxxoooxxxxxooooooooooxxxxxxxxoooxxxxoooooooxxxxxoooooooooooxxxxxxxxxooooooxxxxooooooooxxxxxxxooooooxxxxxxxxoooooxxxxoooooooxxxxooooooooooooooooxxxxxxxxxxxxooooooooxxxxxoooooxxxxxxxxoooxxxxxxxxxxoooooooooooooooooooooooooooooooooooooooxxxxxxxxxxxxxxxxxoooooooooooooxxxxxxooooooooooooooooooooooooooooxxxxxxxxxxxxooooooooooooooooooooooooooxxxxxxxxxxxxxxooooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooooooooooooooooooooooooxxxxxxxxxxxxxxooooooooxxxxxxxxxooooooooooxxxxxxxxxxxxxxxxxxxxxooooooxxxxxoooooooooooooooooooooooooxxxxxxxxoooooooxxxxoooooooooooooxxxxxxxxxxxxxxxxxoooooooxxxxxxxooooooooxxxxxxxxxoooooooxxxxxxxxxxxxxxxoooooooooooxxxxoooooooooxxxoooooooxxxoooxxxoooxxxxxxxxxxxxxooooooooooooooooooxxxxoooooooxxxxxxxxxxxxxxoooxxxxxxxxooooooxxxxxxxxxxxxxxooooooxxxoooooooooooooxxxxxxoooooxxxoooxxxxxoooooooooooxxxxxxxxxxxxxxxxxxxxxxoooxxxxxxxxooooxxxxxooooooxxxxxxxxxxxxxoooxxxxxoooxxxxxxooooxxxxxxxxxxxooooooooooxxxxxxxoooooxxxxxxxxxxxxxxxxxoooooooxxxxoooooxxxxxxxooooooxxxxxxxxxxxxxxxxxxxxxxoooooxxxxxxoooxxxxooooxxxxxxxxxxxxxoooxxxxxoooxxxxxxxxxxxoooooooooooxxxxxxxxxxxooooxxxooooooooxxxxxooooxxxxxxxxxxxooooxxxxooooooooxxxxxxxooooooooooxxxxooooxxxxxxoooxxxxxxxxxxxxxxoooxxxxxxxxxxxxxoooxxxxxxxxxxxoooooxxxxooooooooooooooooooooooooooooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxooooooxxxxoooooooooooxxxxxxxxoooooxxxxxxxxxxooooooooxxxxxxxxxxxxxoooooooooooooxxxxxxxoooooxxxooooooxxxxxxxxxoooxxxoooxxxoooooooooooxxxxxxxxxxxxxxxxxxxxxxooooooxxxxoooxxxxxxxxxxxxxoooooxxxoooxxxxxooooooxxxxxxxxxxxxxxxooooooxxxxxxooooooxxxxxoooxxxxxxxoooxxxxoooooooxxxxxxoooooooooxxxxxxxoooooooooxxxxxxxxxooooooxxxoooxxxxxxooooooooooxxxoooooxxxxxxxxxxxxxxxxxxxxxxxxoooxxxxxoooooxxxoooooooooooxxxxxxxxooooooooxxxxxxoooxxxxxxoooooxxxxxooooxxxxxxxxxxxxoooooooxxxxooooooooooooooxxxxxxxxooooxxxxxxxxxxxxxxooooooooxxxxoooooooooooxxxoooxxxxxxxxooooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooooooxxxxxxxxxxxoooooxxxxxxxxxxxxxxoooxxxxxxxoooxxxxxxoooooooooooxxxxxxxxoooooooooooxxxxxxxxxxooooooooooooooooxxxxxxxxxoooxxxxoooooxxxxxxoooxxxxooooxxxxxxxxoooxxxooooxxxooooooxxxxxxxxxxxxxxxxoooxxxxoooxxxoooxxxxxxoooooooxxxoooooooxxxoooxxxxooooooxxxoooooooooooooooooxxxooooooooxxxxxxxxxxxxxxxoooxxxooooooooooooxxxxxoooooxxxxooooooooxxxxxxoooooxxxxxxxxoooooooooooooxxxxxxooooooooooooooooxxxxxooooxxxxxxxxxooooooooooxxxoooooooxxxxxxxxxxxoooooooooooxxxxooooooooooooxxxxxxxxxxxxxxxxxxooooooooooxxxxxxxxoooooxxxoooooooooooooxxxxxxxxxxoooxxxxxxxxxxxoooxxxxxxxxxxxxoooooxxxoooooooooooxxxxxxxxxoooxxxxxxxxxoooxxxxxooooxxxxxxxxxxxooooooxxxxxooooooxxxxoooooooooooxxxxxxxxxxooooooxxxxoooxxxxooooooxxxxxxxxxxxxxxxxoooxxxxxxxxxxooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooxxxoooooxxxxxxxoooxxxooooxxxxxxxooooooxxxxxxxxxxoooooooxxxxxxoooxxxooooxxxoooxxxxoooooooxxxxxxxxxxxxxxxxxxxxxxooooooxxxxooooooxxxxoooooooxxxxxxxooooooooxxxxxxxoooooooxxxxxxxxooooxxxxxxxxxxxxxxxxxxxooooooooxxxxxooooxxxxxxxxoooooooxxxxoooooxxxxxxxoooooxxxooooooooxxxxxxxxxxooooooooxxxoooooooooooxxxxxooooooooxxxxxxoooooooooooooooooooooooxxxxxxxxxxxxxxxxxxoooooxxxxxxxxooooooxxxxoooxxxxxxxooooxxxxooooooooxxxxxxxxxxooooxxxxxxxxooooooooooxxxxxxooooxxxxoooooooooooooooxxxoooooooooxxxxoooooxxxoooooxxxxxxxxoooooooooxxxxooooxxxxxxoooooxxxxxxxxxxoooxxxxxxxxxxxoooxxxoooooooooooooooooooxxxooooooxxxxxxxxxxxoooxxxxooooooooooxxxxxxxxxooooxxxxxxxxxxxxxooooooxxxxxxxxxxxoooxxxxxoooxxxooooooooooooxxxxxxxoooxxxxxxxxxxxxxxxxxxxxxxxxxxxxxoooxxxxxxxxxxoooxxxooooooooxxxoooxxxxxxxoooxxxxxxxxooooxxxxooooxxxxxoooxxxoooooooooxxxxxxxoooxxxxxxxxxxxxooooxxxxoooooxxxxxxooooxxxxxooooooooooooooxxxxoooooooxxxxxxxxxxxxxxxxxxooooxxxxxooooooooooxxxxxoooooxxxxxoooxxxxxxooooooooooxxxxxxxxxxxxxxxxxxooooooxxxxxxxoooxxxxxxxxxxxxxxxxxxxoooooxxxxoooooxxxxxxxoooooxxxooooooxxxxooooxxxxoooxxxoooooooxxxxxxooooooooooxxxxxooooooooooooxxxxooooooooooooxxxxxxxooooooooooooxxxxxoooooooooooooooxxxxxxooooxxxxxoooooooooooxxxxxxxxoooxxxxxoooxxxxxxxxxxxxxxoooxxxxooooxxxxxxxxxxxxxxxooooooxxx\n"
     ]
    }
   ],
   "source": [
    "# Load ticker data into dataframe\n",
    "aapl_df = pd.read_csv('./data/aapl_extended.csv')\n",
    "\n",
    "# Rename columns for use in PnF Library\n",
    "aapl_df.columns = ['Date','Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']\n",
    "print(aapl_df.head())\n",
    "\n",
    "# Create PnF Chart from dataframe\n",
    "dfi = DataframeInstrument('AAPL', dataframe=aapl_df)\n",
    "chart = PFChart(dfi, duration=20.0) #Duration = years of data\n",
    "chart.create_chart()\n",
    "\n",
    "# Create sequence        \n",
    "pnf_sequence = generate_pnf_char_sequence(chart._chart_meta_data)\n",
    "\n",
    "print(len(pnf_sequence), pnf_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "Want to map our input characters to integers and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(pnf_sequence))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[char] for char in pnf_sequence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    # Init one-hot array to correct size\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    #Fill appropriate elements with 1s\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshape to get back to desired shape\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test and Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2309 9236\n"
     ]
    }
   ],
   "source": [
    "test_prop = 0.2\n",
    "test_end_idx = int(len(encoded) * test_prop)\n",
    "\n",
    "test_data = encoded[:test_end_idx]\n",
    "train_data = encoded[test_end_idx:]\n",
    "\n",
    "print(len(test_data), len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, lstm_size=256, lstm_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Create Char Dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## Define LSTM Layer\n",
    "        self.lstm = nn.LSTM(len(self.chars), lstm_size, lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## Define Dropout Layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define Final Desnse Output Layer\n",
    "        self.fc = nn.Linear(lstm_size, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Input -> LSTM\n",
    "        lstm_out, hidden = self.lstm(nn_input, hidden)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\n",
    "        \n",
    "        # LSTM -> Dense -> Dropout\n",
    "        dense_out = self.dropout(self.fc(lstm_out))\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return dense_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes lstm_layers x batch_size x lstm_size,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                      weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(2, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Determine if we can run on GPU or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_size = 512\n",
    "lstm_layers = 2\n",
    "\n",
    "model = CharRNN(chars, lstm_size, lstm_layers)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Epoch: 1/5... Step: 10... Loss: 0.4477... Val Loss: 0.4364\n",
      "Epoch: 1/5... Step: 20... Loss: 0.5068... Val Loss: 0.4232\n",
      "Epoch: 1/5... Step: 30... Loss: 0.4721... Val Loss: 0.4756\n",
      "Epoch: 1/5... Step: 40... Loss: 0.4883... Val Loss: 0.4289\n",
      "Starting Epoch 2\n",
      "Epoch: 2/5... Step: 50... Loss: 0.4342... Val Loss: 0.4295\n",
      "Epoch: 2/5... Step: 60... Loss: 0.4771... Val Loss: 0.4090\n",
      "Epoch: 2/5... Step: 70... Loss: 0.4494... Val Loss: 0.4253\n",
      "Epoch: 2/5... Step: 80... Loss: 0.4805... Val Loss: 0.4249\n",
      "Epoch: 2/5... Step: 90... Loss: 0.4565... Val Loss: 0.4239\n",
      "Starting Epoch 3\n",
      "Epoch: 3/5... Step: 100... Loss: 0.4468... Val Loss: 0.4414\n",
      "Epoch: 3/5... Step: 110... Loss: 0.4776... Val Loss: 0.4590\n",
      "Epoch: 3/5... Step: 120... Loss: 0.4535... Val Loss: 0.4192\n",
      "Epoch: 3/5... Step: 130... Loss: 0.4464... Val Loss: 0.4250\n",
      "Starting Epoch 4\n",
      "Epoch: 4/5... Step: 140... Loss: 0.4033... Val Loss: 0.4207\n",
      "Epoch: 4/5... Step: 150... Loss: 0.5081... Val Loss: 0.4285\n",
      "Epoch: 4/5... Step: 160... Loss: 0.4243... Val Loss: 0.4324\n",
      "Epoch: 4/5... Step: 170... Loss: 0.4616... Val Loss: 0.4418\n",
      "Epoch: 4/5... Step: 180... Loss: 0.4040... Val Loss: 0.4207\n",
      "Starting Epoch 5\n",
      "Epoch: 5/5... Step: 190... Loss: 0.4723... Val Loss: 0.4221\n",
      "Epoch: 5/5... Step: 200... Loss: 0.4982... Val Loss: 0.4252\n",
      "Epoch: 5/5... Step: 210... Loss: 0.4561... Val Loss: 0.4157\n",
      "Epoch: 5/5... Step: 220... Loss: 0.4180... Val Loss: 0.4480\n",
      "Epoch: 5/5... Step: 230... Loss: 0.4663... Val Loss: 0.4222\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 8\n",
    "sequence_length = 25\n",
    "clip = 5\n",
    "learn_rate = 0.003\n",
    "print_every = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)\n",
    "\n",
    "steps = 0\n",
    "n_chars = len(model.chars)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Starting Epoch {}\".format(epoch+1))\n",
    "    # Set to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through training batches\n",
    "    for train_batch, train_labels in get_batches(train_data, batch_size, sequence_length):\n",
    "        steps += 1\n",
    "        \n",
    "        # Init hidden state\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        # One-hot encode data and convert to tensors\n",
    "        train_batch = one_hot_encode(train_batch, n_chars)\n",
    "        train_batch, train_labels = torch.from_numpy(train_batch), torch.from_numpy(train_labels)\n",
    "        \n",
    "        # Ensure that tensors are on correct devices\n",
    "        train_batch, train_labels = train_batch.to(device), train_labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # Zero-out gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Run data through model and get output/new hidden state\n",
    "        output, hidden = model(train_batch, hidden)\n",
    "        \n",
    "        # Calculate loss and perform backprop -- clip gradients if necessary\n",
    "        loss = criterion(output, train_labels.view(batch_size*sequence_length).long())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        #Take step with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate Model\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            \n",
    "            val_losses = []\n",
    "            \n",
    "            for val_batch, val_labels in get_batches(test_data, batch_size, sequence_length):\n",
    "                # Init hidden state\n",
    "                val_hidden = model.init_hidden(batch_size)\n",
    "                val_hidden = tuple([each.data for each in val_hidden])\n",
    "                \n",
    "                # One-hot encode data and convert to tensors\n",
    "                val_batch = one_hot_encode(val_batch, n_chars)\n",
    "                val_batch, val_labels = torch.from_numpy(val_batch), torch.from_numpy(val_labels)\n",
    "                \n",
    "                # Ensure tensors are on correct devices\n",
    "                val_batch, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                # Run validation data through network\n",
    "                val_output, val_hidden = model(val_batch, val_hidden)\n",
    "                \n",
    "                # Calculate and record loss\n",
    "                val_loss = criterion(val_output, val_labels.view(batch_size*sequence_length).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                # Set back to training mode\n",
    "                model.train()\n",
    "                \n",
    "            # Print out statistics\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step: {}...\".format(steps),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:julie-stav-ws]",
   "language": "python",
   "name": "conda-env-julie-stav-ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
